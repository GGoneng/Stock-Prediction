{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torchmetrics.regression import R2Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_price_1</th>\n",
       "      <th>market_price_2</th>\n",
       "      <th>market_price_3</th>\n",
       "      <th>market_price_4</th>\n",
       "      <th>market_price_5</th>\n",
       "      <th>market_price_6</th>\n",
       "      <th>market_price_7</th>\n",
       "      <th>market_price_8</th>\n",
       "      <th>market_price_9</th>\n",
       "      <th>market_price_10</th>\n",
       "      <th>...</th>\n",
       "      <th>closing_price_1</th>\n",
       "      <th>closing_price_2</th>\n",
       "      <th>closing_price_3</th>\n",
       "      <th>closing_price_4</th>\n",
       "      <th>closing_price_5</th>\n",
       "      <th>closing_price_6</th>\n",
       "      <th>closing_price_7</th>\n",
       "      <th>closing_price_8</th>\n",
       "      <th>closing_price_9</th>\n",
       "      <th>closing_price_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32,650</td>\n",
       "      <td>32,700</td>\n",
       "      <td>32,430</td>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>...</td>\n",
       "      <td>32,735</td>\n",
       "      <td>32,240</td>\n",
       "      <td>32,150</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32,700</td>\n",
       "      <td>32,430</td>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>...</td>\n",
       "      <td>32,240</td>\n",
       "      <td>32,150</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32,430</td>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>31,955</td>\n",
       "      <td>...</td>\n",
       "      <td>32,150</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "      <td>32,175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>31,955</td>\n",
       "      <td>32,120</td>\n",
       "      <td>...</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "      <td>32,175</td>\n",
       "      <td>32,285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>31,955</td>\n",
       "      <td>32,120</td>\n",
       "      <td>32,010</td>\n",
       "      <td>...</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "      <td>32,175</td>\n",
       "      <td>32,285</td>\n",
       "      <td>32,170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151496</th>\n",
       "      <td>9,560</td>\n",
       "      <td>9,535</td>\n",
       "      <td>9,600</td>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>...</td>\n",
       "      <td>9,520</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151497</th>\n",
       "      <td>9,535</td>\n",
       "      <td>9,600</td>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>...</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151498</th>\n",
       "      <td>9,600</td>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,510</td>\n",
       "      <td>...</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151499</th>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,510</td>\n",
       "      <td>9,580</td>\n",
       "      <td>...</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,585</td>\n",
       "      <td>9,660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151500</th>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,510</td>\n",
       "      <td>9,580</td>\n",
       "      <td>9,585</td>\n",
       "      <td>...</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,585</td>\n",
       "      <td>9,660</td>\n",
       "      <td>9,700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151501 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_price_1 market_price_2 market_price_3 market_price_4  \\\n",
       "0              32,650         32,700         32,430         32,325   \n",
       "1              32,700         32,430         32,325         31,775   \n",
       "2              32,430         32,325         31,775         31,825   \n",
       "3              32,325         31,775         31,825         31,810   \n",
       "4              31,775         31,825         31,810         31,700   \n",
       "...               ...            ...            ...            ...   \n",
       "151496          9,560          9,535          9,600          9,830   \n",
       "151497          9,535          9,600          9,830          9,810   \n",
       "151498          9,600          9,830          9,810          9,905   \n",
       "151499          9,830          9,810          9,905          9,745   \n",
       "151500          9,810          9,905          9,745          9,635   \n",
       "\n",
       "       market_price_5 market_price_6 market_price_7 market_price_8  \\\n",
       "0              31,775         31,825         31,810         31,700   \n",
       "1              31,825         31,810         31,700         31,210   \n",
       "2              31,810         31,700         31,210         32,040   \n",
       "3              31,700         31,210         32,040         31,835   \n",
       "4              31,210         32,040         31,835         31,955   \n",
       "...               ...            ...            ...            ...   \n",
       "151496          9,810          9,905          9,745          9,635   \n",
       "151497          9,905          9,745          9,635          9,675   \n",
       "151498          9,745          9,635          9,675          9,690   \n",
       "151499          9,635          9,675          9,690          9,655   \n",
       "151500          9,675          9,690          9,655          9,510   \n",
       "\n",
       "       market_price_9 market_price_10  ... closing_price_1 closing_price_2  \\\n",
       "0              31,210          32,040  ...          32,735          32,240   \n",
       "1              32,040          31,835  ...          32,240          32,150   \n",
       "2              31,835          31,955  ...          32,150          31,805   \n",
       "3              31,955          32,120  ...          31,805          32,060   \n",
       "4              32,120          32,010  ...          32,060          31,285   \n",
       "...               ...             ...  ...             ...             ...   \n",
       "151496          9,675           9,690  ...           9,520           9,545   \n",
       "151497          9,690           9,655  ...           9,545           9,810   \n",
       "151498          9,655           9,510  ...           9,810           9,835   \n",
       "151499          9,510           9,580  ...           9,835           9,745   \n",
       "151500          9,580           9,585  ...           9,745           9,665   \n",
       "\n",
       "       closing_price_3 closing_price_4 closing_price_5 closing_price_6  \\\n",
       "0               32,150          31,805          32,060          31,285   \n",
       "1               31,805          32,060          31,285          31,690   \n",
       "2               32,060          31,285          31,690          31,580   \n",
       "3               31,285          31,690          31,580          31,940   \n",
       "4               31,690          31,580          31,940          31,705   \n",
       "...                ...             ...             ...             ...   \n",
       "151496           9,810           9,835           9,745           9,665   \n",
       "151497           9,835           9,745           9,665           9,620   \n",
       "151498           9,745           9,665           9,620           9,690   \n",
       "151499           9,665           9,620           9,690           9,655   \n",
       "151500           9,620           9,690           9,655           9,605   \n",
       "\n",
       "       closing_price_7 closing_price_8 closing_price_9 closing_price_10  \n",
       "0               31,690          31,580          31,940           31,705  \n",
       "1               31,580          31,940          31,705           31,780  \n",
       "2               31,940          31,705          31,780           32,175  \n",
       "3               31,705          31,780          32,175           32,285  \n",
       "4               31,780          32,175          32,285           32,170  \n",
       "...                ...             ...             ...              ...  \n",
       "151496           9,620           9,690           9,655            9,605  \n",
       "151497           9,690           9,655           9,605            9,545  \n",
       "151498           9,655           9,605           9,545            9,585  \n",
       "151499           9,605           9,545           9,585            9,660  \n",
       "151500           9,545           9,585           9,660            9,700  \n",
       "\n",
       "[151501 rows x 50 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = './Data/ETF_DATA/train.csv'\n",
    "\n",
    "data_df = pd.read_csv(DATA_PATH, index_col = 0)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['market_price_1', 'market_price_2', 'market_price_3', 'market_price_4',\n",
       "       'market_price_5', 'market_price_6', 'market_price_7', 'market_price_8',\n",
       "       'market_price_9', 'market_price_10', 'high_price_1', 'high_price_2',\n",
       "       'high_price_3', 'high_price_4', 'high_price_5', 'high_price_6',\n",
       "       'high_price_7', 'high_price_8', 'high_price_9', 'high_price_10',\n",
       "       'low_price_1', 'low_price_2', 'low_price_3', 'low_price_4',\n",
       "       'low_price_5', 'low_price_6', 'low_price_7', 'low_price_8',\n",
       "       'low_price_9', 'low_price_10', 'volume_1', 'volume_2', 'volume_3',\n",
       "       'volume_4', 'volume_5', 'volume_6', 'volume_7', 'volume_8', 'volume_9',\n",
       "       'volume_10', 'closing_price_1', 'closing_price_2', 'closing_price_3',\n",
       "       'closing_price_4', 'closing_price_5', 'closing_price_6',\n",
       "       'closing_price_7', 'closing_price_8', 'closing_price_9',\n",
       "       'closing_price_10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(['market_price_10', 'high_price_10', 'low_price_10', 'volume_10'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dropna(inplace = True, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_price_1</th>\n",
       "      <th>market_price_2</th>\n",
       "      <th>market_price_3</th>\n",
       "      <th>market_price_4</th>\n",
       "      <th>market_price_5</th>\n",
       "      <th>market_price_6</th>\n",
       "      <th>market_price_7</th>\n",
       "      <th>market_price_8</th>\n",
       "      <th>market_price_9</th>\n",
       "      <th>high_price_1</th>\n",
       "      <th>...</th>\n",
       "      <th>closing_price_1</th>\n",
       "      <th>closing_price_2</th>\n",
       "      <th>closing_price_3</th>\n",
       "      <th>closing_price_4</th>\n",
       "      <th>closing_price_5</th>\n",
       "      <th>closing_price_6</th>\n",
       "      <th>closing_price_7</th>\n",
       "      <th>closing_price_8</th>\n",
       "      <th>closing_price_9</th>\n",
       "      <th>closing_price_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32,650</td>\n",
       "      <td>32,700</td>\n",
       "      <td>32,430</td>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,910</td>\n",
       "      <td>...</td>\n",
       "      <td>32,735</td>\n",
       "      <td>32,240</td>\n",
       "      <td>32,150</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32,700</td>\n",
       "      <td>32,430</td>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>32,995</td>\n",
       "      <td>...</td>\n",
       "      <td>32,240</td>\n",
       "      <td>32,150</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32,430</td>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>32,440</td>\n",
       "      <td>...</td>\n",
       "      <td>32,150</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "      <td>32,175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32,325</td>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>31,955</td>\n",
       "      <td>32,445</td>\n",
       "      <td>...</td>\n",
       "      <td>31,805</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "      <td>32,175</td>\n",
       "      <td>32,285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31,775</td>\n",
       "      <td>31,825</td>\n",
       "      <td>31,810</td>\n",
       "      <td>31,700</td>\n",
       "      <td>31,210</td>\n",
       "      <td>32,040</td>\n",
       "      <td>31,835</td>\n",
       "      <td>31,955</td>\n",
       "      <td>32,120</td>\n",
       "      <td>31,905</td>\n",
       "      <td>...</td>\n",
       "      <td>32,060</td>\n",
       "      <td>31,285</td>\n",
       "      <td>31,690</td>\n",
       "      <td>31,580</td>\n",
       "      <td>31,940</td>\n",
       "      <td>31,705</td>\n",
       "      <td>31,780</td>\n",
       "      <td>32,175</td>\n",
       "      <td>32,285</td>\n",
       "      <td>32,170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151496</th>\n",
       "      <td>9,560</td>\n",
       "      <td>9,535</td>\n",
       "      <td>9,600</td>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,630</td>\n",
       "      <td>...</td>\n",
       "      <td>9,520</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151497</th>\n",
       "      <td>9,535</td>\n",
       "      <td>9,600</td>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,535</td>\n",
       "      <td>...</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151498</th>\n",
       "      <td>9,600</td>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,645</td>\n",
       "      <td>...</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151499</th>\n",
       "      <td>9,830</td>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,510</td>\n",
       "      <td>9,835</td>\n",
       "      <td>...</td>\n",
       "      <td>9,835</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,585</td>\n",
       "      <td>9,660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151500</th>\n",
       "      <td>9,810</td>\n",
       "      <td>9,905</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,635</td>\n",
       "      <td>9,675</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,510</td>\n",
       "      <td>9,580</td>\n",
       "      <td>9,840</td>\n",
       "      <td>...</td>\n",
       "      <td>9,745</td>\n",
       "      <td>9,665</td>\n",
       "      <td>9,620</td>\n",
       "      <td>9,690</td>\n",
       "      <td>9,655</td>\n",
       "      <td>9,605</td>\n",
       "      <td>9,545</td>\n",
       "      <td>9,585</td>\n",
       "      <td>9,660</td>\n",
       "      <td>9,700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151482 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_price_1 market_price_2 market_price_3 market_price_4  \\\n",
       "0              32,650         32,700         32,430         32,325   \n",
       "1              32,700         32,430         32,325         31,775   \n",
       "2              32,430         32,325         31,775         31,825   \n",
       "3              32,325         31,775         31,825         31,810   \n",
       "4              31,775         31,825         31,810         31,700   \n",
       "...               ...            ...            ...            ...   \n",
       "151496          9,560          9,535          9,600          9,830   \n",
       "151497          9,535          9,600          9,830          9,810   \n",
       "151498          9,600          9,830          9,810          9,905   \n",
       "151499          9,830          9,810          9,905          9,745   \n",
       "151500          9,810          9,905          9,745          9,635   \n",
       "\n",
       "       market_price_5 market_price_6 market_price_7 market_price_8  \\\n",
       "0              31,775         31,825         31,810         31,700   \n",
       "1              31,825         31,810         31,700         31,210   \n",
       "2              31,810         31,700         31,210         32,040   \n",
       "3              31,700         31,210         32,040         31,835   \n",
       "4              31,210         32,040         31,835         31,955   \n",
       "...               ...            ...            ...            ...   \n",
       "151496          9,810          9,905          9,745          9,635   \n",
       "151497          9,905          9,745          9,635          9,675   \n",
       "151498          9,745          9,635          9,675          9,690   \n",
       "151499          9,635          9,675          9,690          9,655   \n",
       "151500          9,675          9,690          9,655          9,510   \n",
       "\n",
       "       market_price_9 high_price_1  ... closing_price_1 closing_price_2  \\\n",
       "0              31,210       32,910  ...          32,735          32,240   \n",
       "1              32,040       32,995  ...          32,240          32,150   \n",
       "2              31,835       32,440  ...          32,150          31,805   \n",
       "3              31,955       32,445  ...          31,805          32,060   \n",
       "4              32,120       31,905  ...          32,060          31,285   \n",
       "...               ...          ...  ...             ...             ...   \n",
       "151496          9,675        9,630  ...           9,520           9,545   \n",
       "151497          9,690        9,535  ...           9,545           9,810   \n",
       "151498          9,655        9,645  ...           9,810           9,835   \n",
       "151499          9,510        9,835  ...           9,835           9,745   \n",
       "151500          9,580        9,840  ...           9,745           9,665   \n",
       "\n",
       "       closing_price_3 closing_price_4 closing_price_5 closing_price_6  \\\n",
       "0               32,150          31,805          32,060          31,285   \n",
       "1               31,805          32,060          31,285          31,690   \n",
       "2               32,060          31,285          31,690          31,580   \n",
       "3               31,285          31,690          31,580          31,940   \n",
       "4               31,690          31,580          31,940          31,705   \n",
       "...                ...             ...             ...             ...   \n",
       "151496           9,810           9,835           9,745           9,665   \n",
       "151497           9,835           9,745           9,665           9,620   \n",
       "151498           9,745           9,665           9,620           9,690   \n",
       "151499           9,665           9,620           9,690           9,655   \n",
       "151500           9,620           9,690           9,655           9,605   \n",
       "\n",
       "       closing_price_7 closing_price_8 closing_price_9 closing_price_10  \n",
       "0               31,690          31,580          31,940           31,705  \n",
       "1               31,580          31,940          31,705           31,780  \n",
       "2               31,940          31,705          31,780           32,175  \n",
       "3               31,705          31,780          32,175           32,285  \n",
       "4               31,780          32,175          32,285           32,170  \n",
       "...                ...             ...             ...              ...  \n",
       "151496           9,620           9,690           9,655            9,605  \n",
       "151497           9,690           9,655           9,605            9,545  \n",
       "151498           9,655           9,605           9,545            9,585  \n",
       "151499           9,605           9,545           9,585            9,660  \n",
       "151500           9,545           9,585           9,660            9,700  \n",
       "\n",
       "[151482 rows x 46 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_df[data_df.columns[:-1]]\n",
    "target = data_df[data_df.columns[-1:]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    random_state = 1,\n",
    "                                                    test_size = 0.1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train,\n",
    "                                                  random_state = 1,\n",
    "                                                  test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closing_price_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66565</th>\n",
       "      <td>10,240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137623</th>\n",
       "      <td>11,560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115806</th>\n",
       "      <td>16,750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121920</th>\n",
       "      <td>17,445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66264</th>\n",
       "      <td>13,055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143561</th>\n",
       "      <td>6,375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114561</th>\n",
       "      <td>16,355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37174</th>\n",
       "      <td>10,140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>8,925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112909</th>\n",
       "      <td>6,410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122699 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       closing_price_10\n",
       "66565            10,240\n",
       "137623           11,560\n",
       "115806           16,750\n",
       "121920           17,445\n",
       "66264            13,055\n",
       "...                 ...\n",
       "143561            6,375\n",
       "114561           16,355\n",
       "37174            10,140\n",
       "56419             8,925\n",
       "112909            6,410\n",
       "\n",
       "[122699 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_price_1</th>\n",
       "      <th>market_price_2</th>\n",
       "      <th>market_price_3</th>\n",
       "      <th>market_price_4</th>\n",
       "      <th>market_price_5</th>\n",
       "      <th>market_price_6</th>\n",
       "      <th>market_price_7</th>\n",
       "      <th>market_price_8</th>\n",
       "      <th>market_price_9</th>\n",
       "      <th>high_price_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_9</th>\n",
       "      <th>closing_price_1</th>\n",
       "      <th>closing_price_2</th>\n",
       "      <th>closing_price_3</th>\n",
       "      <th>closing_price_4</th>\n",
       "      <th>closing_price_5</th>\n",
       "      <th>closing_price_6</th>\n",
       "      <th>closing_price_7</th>\n",
       "      <th>closing_price_8</th>\n",
       "      <th>closing_price_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66565</th>\n",
       "      <td>10,665</td>\n",
       "      <td>10,570</td>\n",
       "      <td>10,610</td>\n",
       "      <td>10,645</td>\n",
       "      <td>10,630</td>\n",
       "      <td>10,585</td>\n",
       "      <td>10,520</td>\n",
       "      <td>10,470</td>\n",
       "      <td>10,420</td>\n",
       "      <td>10,665</td>\n",
       "      <td>...</td>\n",
       "      <td>485,711</td>\n",
       "      <td>10,600</td>\n",
       "      <td>10,645</td>\n",
       "      <td>10,630</td>\n",
       "      <td>10,585</td>\n",
       "      <td>10,580</td>\n",
       "      <td>10,460</td>\n",
       "      <td>10,420</td>\n",
       "      <td>10,380</td>\n",
       "      <td>10,315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137623</th>\n",
       "      <td>11,305</td>\n",
       "      <td>11,430</td>\n",
       "      <td>11,420</td>\n",
       "      <td>11,415</td>\n",
       "      <td>11,500</td>\n",
       "      <td>11,605</td>\n",
       "      <td>11,620</td>\n",
       "      <td>11,635</td>\n",
       "      <td>11,610</td>\n",
       "      <td>11,415</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>11,465</td>\n",
       "      <td>11,425</td>\n",
       "      <td>11,395</td>\n",
       "      <td>11,575</td>\n",
       "      <td>11,620</td>\n",
       "      <td>11,540</td>\n",
       "      <td>11,605</td>\n",
       "      <td>11,610</td>\n",
       "      <td>11,570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115806</th>\n",
       "      <td>15,295</td>\n",
       "      <td>15,400</td>\n",
       "      <td>15,155</td>\n",
       "      <td>16,045</td>\n",
       "      <td>16,260</td>\n",
       "      <td>16,915</td>\n",
       "      <td>17,640</td>\n",
       "      <td>17,085</td>\n",
       "      <td>16,965</td>\n",
       "      <td>15,400</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>15,000</td>\n",
       "      <td>15,430</td>\n",
       "      <td>15,945</td>\n",
       "      <td>16,295</td>\n",
       "      <td>17,530</td>\n",
       "      <td>17,045</td>\n",
       "      <td>16,820</td>\n",
       "      <td>17,120</td>\n",
       "      <td>16,750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121920</th>\n",
       "      <td>17,870</td>\n",
       "      <td>17,915</td>\n",
       "      <td>17,845</td>\n",
       "      <td>18,015</td>\n",
       "      <td>17,945</td>\n",
       "      <td>17,880</td>\n",
       "      <td>17,870</td>\n",
       "      <td>17,685</td>\n",
       "      <td>17,685</td>\n",
       "      <td>17,870</td>\n",
       "      <td>...</td>\n",
       "      <td>2,077</td>\n",
       "      <td>17,845</td>\n",
       "      <td>17,845</td>\n",
       "      <td>17,945</td>\n",
       "      <td>17,905</td>\n",
       "      <td>17,880</td>\n",
       "      <td>17,715</td>\n",
       "      <td>17,690</td>\n",
       "      <td>17,650</td>\n",
       "      <td>17,705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66264</th>\n",
       "      <td>13,150</td>\n",
       "      <td>13,155</td>\n",
       "      <td>13,285</td>\n",
       "      <td>13,275</td>\n",
       "      <td>13,230</td>\n",
       "      <td>13,225</td>\n",
       "      <td>13,155</td>\n",
       "      <td>13,200</td>\n",
       "      <td>13,290</td>\n",
       "      <td>13,150</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>13,255</td>\n",
       "      <td>13,275</td>\n",
       "      <td>13,265</td>\n",
       "      <td>13,235</td>\n",
       "      <td>13,005</td>\n",
       "      <td>12,975</td>\n",
       "      <td>13,005</td>\n",
       "      <td>12,900</td>\n",
       "      <td>12,945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143561</th>\n",
       "      <td>6,290</td>\n",
       "      <td>6,355</td>\n",
       "      <td>6,300</td>\n",
       "      <td>6,300</td>\n",
       "      <td>6,280</td>\n",
       "      <td>6,355</td>\n",
       "      <td>6,355</td>\n",
       "      <td>6,370</td>\n",
       "      <td>6,360</td>\n",
       "      <td>6,360</td>\n",
       "      <td>...</td>\n",
       "      <td>6,117</td>\n",
       "      <td>6,275</td>\n",
       "      <td>6,245</td>\n",
       "      <td>6,250</td>\n",
       "      <td>6,340</td>\n",
       "      <td>6,395</td>\n",
       "      <td>6,320</td>\n",
       "      <td>6,340</td>\n",
       "      <td>6,405</td>\n",
       "      <td>6,385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114561</th>\n",
       "      <td>16,380</td>\n",
       "      <td>16,300</td>\n",
       "      <td>16,130</td>\n",
       "      <td>16,175</td>\n",
       "      <td>16,150</td>\n",
       "      <td>16,165</td>\n",
       "      <td>16,175</td>\n",
       "      <td>16,040</td>\n",
       "      <td>16,160</td>\n",
       "      <td>16,380</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16,300</td>\n",
       "      <td>16,130</td>\n",
       "      <td>16,175</td>\n",
       "      <td>16,150</td>\n",
       "      <td>16,165</td>\n",
       "      <td>16,175</td>\n",
       "      <td>16,040</td>\n",
       "      <td>16,160</td>\n",
       "      <td>16,170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37174</th>\n",
       "      <td>9,830</td>\n",
       "      <td>9,915</td>\n",
       "      <td>10,055</td>\n",
       "      <td>10,170</td>\n",
       "      <td>10,015</td>\n",
       "      <td>10,125</td>\n",
       "      <td>10,040</td>\n",
       "      <td>10,085</td>\n",
       "      <td>10,170</td>\n",
       "      <td>9,915</td>\n",
       "      <td>...</td>\n",
       "      <td>6,895</td>\n",
       "      <td>10,095</td>\n",
       "      <td>10,020</td>\n",
       "      <td>10,015</td>\n",
       "      <td>10,000</td>\n",
       "      <td>10,040</td>\n",
       "      <td>10,115</td>\n",
       "      <td>10,135</td>\n",
       "      <td>10,175</td>\n",
       "      <td>10,160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>8,715</td>\n",
       "      <td>8,750</td>\n",
       "      <td>8,870</td>\n",
       "      <td>8,815</td>\n",
       "      <td>8,905</td>\n",
       "      <td>8,875</td>\n",
       "      <td>8,890</td>\n",
       "      <td>8,745</td>\n",
       "      <td>8,750</td>\n",
       "      <td>8,780</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>8,845</td>\n",
       "      <td>8,990</td>\n",
       "      <td>8,920</td>\n",
       "      <td>8,825</td>\n",
       "      <td>8,935</td>\n",
       "      <td>8,770</td>\n",
       "      <td>8,735</td>\n",
       "      <td>8,885</td>\n",
       "      <td>8,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112909</th>\n",
       "      <td>7,105</td>\n",
       "      <td>6,960</td>\n",
       "      <td>6,700</td>\n",
       "      <td>6,615</td>\n",
       "      <td>6,700</td>\n",
       "      <td>6,585</td>\n",
       "      <td>6,560</td>\n",
       "      <td>6,485</td>\n",
       "      <td>6,455</td>\n",
       "      <td>7,105</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>6,800</td>\n",
       "      <td>6,670</td>\n",
       "      <td>6,585</td>\n",
       "      <td>6,680</td>\n",
       "      <td>6,575</td>\n",
       "      <td>6,575</td>\n",
       "      <td>6,475</td>\n",
       "      <td>6,410</td>\n",
       "      <td>6,470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122699 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_price_1 market_price_2 market_price_3 market_price_4  \\\n",
       "66565          10,665         10,570         10,610         10,645   \n",
       "137623         11,305         11,430         11,420         11,415   \n",
       "115806         15,295         15,400         15,155         16,045   \n",
       "121920         17,870         17,915         17,845         18,015   \n",
       "66264          13,150         13,155         13,285         13,275   \n",
       "...               ...            ...            ...            ...   \n",
       "143561          6,290          6,355          6,300          6,300   \n",
       "114561         16,380         16,300         16,130         16,175   \n",
       "37174           9,830          9,915         10,055         10,170   \n",
       "56419           8,715          8,750          8,870          8,815   \n",
       "112909          7,105          6,960          6,700          6,615   \n",
       "\n",
       "       market_price_5 market_price_6 market_price_7 market_price_8  \\\n",
       "66565          10,630         10,585         10,520         10,470   \n",
       "137623         11,500         11,605         11,620         11,635   \n",
       "115806         16,260         16,915         17,640         17,085   \n",
       "121920         17,945         17,880         17,870         17,685   \n",
       "66264          13,230         13,225         13,155         13,200   \n",
       "...               ...            ...            ...            ...   \n",
       "143561          6,280          6,355          6,355          6,370   \n",
       "114561         16,150         16,165         16,175         16,040   \n",
       "37174          10,015         10,125         10,040         10,085   \n",
       "56419           8,905          8,875          8,890          8,745   \n",
       "112909          6,700          6,585          6,560          6,485   \n",
       "\n",
       "       market_price_9 high_price_1  ... volume_9 closing_price_1  \\\n",
       "66565          10,420       10,665  ...  485,711          10,600   \n",
       "137623         11,610       11,415  ...       16          11,465   \n",
       "115806         16,965       15,400  ...       13          15,000   \n",
       "121920         17,685       17,870  ...    2,077          17,845   \n",
       "66264          13,290       13,150  ...       28          13,255   \n",
       "...               ...          ...  ...      ...             ...   \n",
       "143561          6,360        6,360  ...    6,117           6,275   \n",
       "114561         16,160       16,380  ...        1          16,300   \n",
       "37174          10,170        9,915  ...    6,895          10,095   \n",
       "56419           8,750        8,780  ...      200           8,845   \n",
       "112909          6,455        7,105  ...      135           6,800   \n",
       "\n",
       "       closing_price_2 closing_price_3 closing_price_4 closing_price_5  \\\n",
       "66565           10,645          10,630          10,585          10,580   \n",
       "137623          11,425          11,395          11,575          11,620   \n",
       "115806          15,430          15,945          16,295          17,530   \n",
       "121920          17,845          17,945          17,905          17,880   \n",
       "66264           13,275          13,265          13,235          13,005   \n",
       "...                ...             ...             ...             ...   \n",
       "143561           6,245           6,250           6,340           6,395   \n",
       "114561          16,130          16,175          16,150          16,165   \n",
       "37174           10,020          10,015          10,000          10,040   \n",
       "56419            8,990           8,920           8,825           8,935   \n",
       "112909           6,670           6,585           6,680           6,575   \n",
       "\n",
       "       closing_price_6 closing_price_7 closing_price_8 closing_price_9  \n",
       "66565           10,460          10,420          10,380          10,315  \n",
       "137623          11,540          11,605          11,610          11,570  \n",
       "115806          17,045          16,820          17,120          16,750  \n",
       "121920          17,715          17,690          17,650          17,705  \n",
       "66264           12,975          13,005          12,900          12,945  \n",
       "...                ...             ...             ...             ...  \n",
       "143561           6,320           6,340           6,405           6,385  \n",
       "114561          16,175          16,040          16,160          16,170  \n",
       "37174           10,115          10,135          10,175          10,160  \n",
       "56419            8,770           8,735           8,885           8,900  \n",
       "112909           6,575           6,475           6,410           6,470  \n",
       "\n",
       "[122699 rows x 45 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF = featureDF\n",
    "        self.targetDF = targetDF\n",
    "        self.n_rows = self.featureDF.shape[0]\n",
    "        self.n_cols = self.featureDF.shape[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        featureTS = torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS = torch.FloatTensor(self.targetDF.iloc[index].values)\n",
    "\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_in, in_out, out_out, h_ins = [], h_outs = [], dropout_prob = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer = nn.Linear(in_in, h_ins[0] if len(h_ins) else in_out)\n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "\n",
    "        self.hd_layers = nn.ModuleList()\n",
    "        for idx in range(len(h_ins) - 1):\n",
    "            self.hd_layers.append(nn.Linear(h_ins[idx], h_outs[idx + 1]))\n",
    "            self.hd_layers.append(nn.Dropout(p = dropout_prob))\n",
    "        \n",
    "        self.out_layer = nn.Linear(h_outs[-1] if len(h_outs) else in_out, out_out)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        y = F.relu(self.in_layer(input_data))\n",
    "        for linear in self.hd_layers:\n",
    "            y = F.relu(linear(y))\n",
    "        \n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_punctuation(dataframe):\n",
    "    for name in dataframe.columns:\n",
    "        dataframe[name] = dataframe[name].str.replace(',', '')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = del_punctuation(X_train)\n",
    "X_val = del_punctuation(X_val)\n",
    "X_test = del_punctuation(X_test)\n",
    "\n",
    "y_train = del_punctuation(y_train)\n",
    "y_val = del_punctuation(y_val)\n",
    "y_test = del_punctuation(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_price_1</th>\n",
       "      <th>market_price_2</th>\n",
       "      <th>market_price_3</th>\n",
       "      <th>market_price_4</th>\n",
       "      <th>market_price_5</th>\n",
       "      <th>market_price_6</th>\n",
       "      <th>market_price_7</th>\n",
       "      <th>market_price_8</th>\n",
       "      <th>market_price_9</th>\n",
       "      <th>high_price_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_9</th>\n",
       "      <th>closing_price_1</th>\n",
       "      <th>closing_price_2</th>\n",
       "      <th>closing_price_3</th>\n",
       "      <th>closing_price_4</th>\n",
       "      <th>closing_price_5</th>\n",
       "      <th>closing_price_6</th>\n",
       "      <th>closing_price_7</th>\n",
       "      <th>closing_price_8</th>\n",
       "      <th>closing_price_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66565</th>\n",
       "      <td>10665</td>\n",
       "      <td>10570</td>\n",
       "      <td>10610</td>\n",
       "      <td>10645</td>\n",
       "      <td>10630</td>\n",
       "      <td>10585</td>\n",
       "      <td>10520</td>\n",
       "      <td>10470</td>\n",
       "      <td>10420</td>\n",
       "      <td>10665</td>\n",
       "      <td>...</td>\n",
       "      <td>485711</td>\n",
       "      <td>10600</td>\n",
       "      <td>10645</td>\n",
       "      <td>10630</td>\n",
       "      <td>10585</td>\n",
       "      <td>10580</td>\n",
       "      <td>10460</td>\n",
       "      <td>10420</td>\n",
       "      <td>10380</td>\n",
       "      <td>10315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137623</th>\n",
       "      <td>11305</td>\n",
       "      <td>11430</td>\n",
       "      <td>11420</td>\n",
       "      <td>11415</td>\n",
       "      <td>11500</td>\n",
       "      <td>11605</td>\n",
       "      <td>11620</td>\n",
       "      <td>11635</td>\n",
       "      <td>11610</td>\n",
       "      <td>11415</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>11465</td>\n",
       "      <td>11425</td>\n",
       "      <td>11395</td>\n",
       "      <td>11575</td>\n",
       "      <td>11620</td>\n",
       "      <td>11540</td>\n",
       "      <td>11605</td>\n",
       "      <td>11610</td>\n",
       "      <td>11570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115806</th>\n",
       "      <td>15295</td>\n",
       "      <td>15400</td>\n",
       "      <td>15155</td>\n",
       "      <td>16045</td>\n",
       "      <td>16260</td>\n",
       "      <td>16915</td>\n",
       "      <td>17640</td>\n",
       "      <td>17085</td>\n",
       "      <td>16965</td>\n",
       "      <td>15400</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>15000</td>\n",
       "      <td>15430</td>\n",
       "      <td>15945</td>\n",
       "      <td>16295</td>\n",
       "      <td>17530</td>\n",
       "      <td>17045</td>\n",
       "      <td>16820</td>\n",
       "      <td>17120</td>\n",
       "      <td>16750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121920</th>\n",
       "      <td>17870</td>\n",
       "      <td>17915</td>\n",
       "      <td>17845</td>\n",
       "      <td>18015</td>\n",
       "      <td>17945</td>\n",
       "      <td>17880</td>\n",
       "      <td>17870</td>\n",
       "      <td>17685</td>\n",
       "      <td>17685</td>\n",
       "      <td>17870</td>\n",
       "      <td>...</td>\n",
       "      <td>2077</td>\n",
       "      <td>17845</td>\n",
       "      <td>17845</td>\n",
       "      <td>17945</td>\n",
       "      <td>17905</td>\n",
       "      <td>17880</td>\n",
       "      <td>17715</td>\n",
       "      <td>17690</td>\n",
       "      <td>17650</td>\n",
       "      <td>17705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66264</th>\n",
       "      <td>13150</td>\n",
       "      <td>13155</td>\n",
       "      <td>13285</td>\n",
       "      <td>13275</td>\n",
       "      <td>13230</td>\n",
       "      <td>13225</td>\n",
       "      <td>13155</td>\n",
       "      <td>13200</td>\n",
       "      <td>13290</td>\n",
       "      <td>13150</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>13255</td>\n",
       "      <td>13275</td>\n",
       "      <td>13265</td>\n",
       "      <td>13235</td>\n",
       "      <td>13005</td>\n",
       "      <td>12975</td>\n",
       "      <td>13005</td>\n",
       "      <td>12900</td>\n",
       "      <td>12945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143561</th>\n",
       "      <td>6290</td>\n",
       "      <td>6355</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6280</td>\n",
       "      <td>6355</td>\n",
       "      <td>6355</td>\n",
       "      <td>6370</td>\n",
       "      <td>6360</td>\n",
       "      <td>6360</td>\n",
       "      <td>...</td>\n",
       "      <td>6117</td>\n",
       "      <td>6275</td>\n",
       "      <td>6245</td>\n",
       "      <td>6250</td>\n",
       "      <td>6340</td>\n",
       "      <td>6395</td>\n",
       "      <td>6320</td>\n",
       "      <td>6340</td>\n",
       "      <td>6405</td>\n",
       "      <td>6385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114561</th>\n",
       "      <td>16380</td>\n",
       "      <td>16300</td>\n",
       "      <td>16130</td>\n",
       "      <td>16175</td>\n",
       "      <td>16150</td>\n",
       "      <td>16165</td>\n",
       "      <td>16175</td>\n",
       "      <td>16040</td>\n",
       "      <td>16160</td>\n",
       "      <td>16380</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16300</td>\n",
       "      <td>16130</td>\n",
       "      <td>16175</td>\n",
       "      <td>16150</td>\n",
       "      <td>16165</td>\n",
       "      <td>16175</td>\n",
       "      <td>16040</td>\n",
       "      <td>16160</td>\n",
       "      <td>16170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37174</th>\n",
       "      <td>9830</td>\n",
       "      <td>9915</td>\n",
       "      <td>10055</td>\n",
       "      <td>10170</td>\n",
       "      <td>10015</td>\n",
       "      <td>10125</td>\n",
       "      <td>10040</td>\n",
       "      <td>10085</td>\n",
       "      <td>10170</td>\n",
       "      <td>9915</td>\n",
       "      <td>...</td>\n",
       "      <td>6895</td>\n",
       "      <td>10095</td>\n",
       "      <td>10020</td>\n",
       "      <td>10015</td>\n",
       "      <td>10000</td>\n",
       "      <td>10040</td>\n",
       "      <td>10115</td>\n",
       "      <td>10135</td>\n",
       "      <td>10175</td>\n",
       "      <td>10160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>8715</td>\n",
       "      <td>8750</td>\n",
       "      <td>8870</td>\n",
       "      <td>8815</td>\n",
       "      <td>8905</td>\n",
       "      <td>8875</td>\n",
       "      <td>8890</td>\n",
       "      <td>8745</td>\n",
       "      <td>8750</td>\n",
       "      <td>8780</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>8845</td>\n",
       "      <td>8990</td>\n",
       "      <td>8920</td>\n",
       "      <td>8825</td>\n",
       "      <td>8935</td>\n",
       "      <td>8770</td>\n",
       "      <td>8735</td>\n",
       "      <td>8885</td>\n",
       "      <td>8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112909</th>\n",
       "      <td>7105</td>\n",
       "      <td>6960</td>\n",
       "      <td>6700</td>\n",
       "      <td>6615</td>\n",
       "      <td>6700</td>\n",
       "      <td>6585</td>\n",
       "      <td>6560</td>\n",
       "      <td>6485</td>\n",
       "      <td>6455</td>\n",
       "      <td>7105</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>6800</td>\n",
       "      <td>6670</td>\n",
       "      <td>6585</td>\n",
       "      <td>6680</td>\n",
       "      <td>6575</td>\n",
       "      <td>6575</td>\n",
       "      <td>6475</td>\n",
       "      <td>6410</td>\n",
       "      <td>6470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122699 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_price_1 market_price_2 market_price_3 market_price_4  \\\n",
       "66565           10665          10570          10610          10645   \n",
       "137623          11305          11430          11420          11415   \n",
       "115806          15295          15400          15155          16045   \n",
       "121920          17870          17915          17845          18015   \n",
       "66264           13150          13155          13285          13275   \n",
       "...               ...            ...            ...            ...   \n",
       "143561           6290           6355           6300           6300   \n",
       "114561          16380          16300          16130          16175   \n",
       "37174            9830           9915          10055          10170   \n",
       "56419            8715           8750           8870           8815   \n",
       "112909           7105           6960           6700           6615   \n",
       "\n",
       "       market_price_5 market_price_6 market_price_7 market_price_8  \\\n",
       "66565           10630          10585          10520          10470   \n",
       "137623          11500          11605          11620          11635   \n",
       "115806          16260          16915          17640          17085   \n",
       "121920          17945          17880          17870          17685   \n",
       "66264           13230          13225          13155          13200   \n",
       "...               ...            ...            ...            ...   \n",
       "143561           6280           6355           6355           6370   \n",
       "114561          16150          16165          16175          16040   \n",
       "37174           10015          10125          10040          10085   \n",
       "56419            8905           8875           8890           8745   \n",
       "112909           6700           6585           6560           6485   \n",
       "\n",
       "       market_price_9 high_price_1  ... volume_9 closing_price_1  \\\n",
       "66565           10420        10665  ...   485711           10600   \n",
       "137623          11610        11415  ...       16           11465   \n",
       "115806          16965        15400  ...       13           15000   \n",
       "121920          17685        17870  ...     2077           17845   \n",
       "66264           13290        13150  ...       28           13255   \n",
       "...               ...          ...  ...      ...             ...   \n",
       "143561           6360         6360  ...     6117            6275   \n",
       "114561          16160        16380  ...        1           16300   \n",
       "37174           10170         9915  ...     6895           10095   \n",
       "56419            8750         8780  ...      200            8845   \n",
       "112909           6455         7105  ...      135            6800   \n",
       "\n",
       "       closing_price_2 closing_price_3 closing_price_4 closing_price_5  \\\n",
       "66565            10645           10630           10585           10580   \n",
       "137623           11425           11395           11575           11620   \n",
       "115806           15430           15945           16295           17530   \n",
       "121920           17845           17945           17905           17880   \n",
       "66264            13275           13265           13235           13005   \n",
       "...                ...             ...             ...             ...   \n",
       "143561            6245            6250            6340            6395   \n",
       "114561           16130           16175           16150           16165   \n",
       "37174            10020           10015           10000           10040   \n",
       "56419             8990            8920            8825            8935   \n",
       "112909            6670            6585            6680            6575   \n",
       "\n",
       "       closing_price_6 closing_price_7 closing_price_8 closing_price_9  \n",
       "66565            10460           10420           10380           10315  \n",
       "137623           11540           11605           11610           11570  \n",
       "115806           17045           16820           17120           16750  \n",
       "121920           17715           17690           17650           17705  \n",
       "66264            12975           13005           12900           12945  \n",
       "...                ...             ...             ...             ...  \n",
       "143561            6320            6340            6405            6385  \n",
       "114561           16175           16040           16160           16170  \n",
       "37174            10115           10135           10175           10160  \n",
       "56419             8770            8735            8885            8900  \n",
       "112909            6575            6475            6410            6470  \n",
       "\n",
       "[122699 rows x 45 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('int')\n",
    "X_val = X_val.astype('int')\n",
    "X_test = X_test.astype('int')\n",
    "\n",
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15149 entries, 55576 to 94070\n",
      "Data columns (total 45 columns):\n",
      " #   Column           Non-Null Count  Dtype\n",
      "---  ------           --------------  -----\n",
      " 0   market_price_1   15149 non-null  int32\n",
      " 1   market_price_2   15149 non-null  int32\n",
      " 2   market_price_3   15149 non-null  int32\n",
      " 3   market_price_4   15149 non-null  int32\n",
      " 4   market_price_5   15149 non-null  int32\n",
      " 5   market_price_6   15149 non-null  int32\n",
      " 6   market_price_7   15149 non-null  int32\n",
      " 7   market_price_8   15149 non-null  int32\n",
      " 8   market_price_9   15149 non-null  int32\n",
      " 9   high_price_1     15149 non-null  int32\n",
      " 10  high_price_2     15149 non-null  int32\n",
      " 11  high_price_3     15149 non-null  int32\n",
      " 12  high_price_4     15149 non-null  int32\n",
      " 13  high_price_5     15149 non-null  int32\n",
      " 14  high_price_6     15149 non-null  int32\n",
      " 15  high_price_7     15149 non-null  int32\n",
      " 16  high_price_8     15149 non-null  int32\n",
      " 17  high_price_9     15149 non-null  int32\n",
      " 18  low_price_1      15149 non-null  int32\n",
      " 19  low_price_2      15149 non-null  int32\n",
      " 20  low_price_3      15149 non-null  int32\n",
      " 21  low_price_4      15149 non-null  int32\n",
      " 22  low_price_5      15149 non-null  int32\n",
      " 23  low_price_6      15149 non-null  int32\n",
      " 24  low_price_7      15149 non-null  int32\n",
      " 25  low_price_8      15149 non-null  int32\n",
      " 26  low_price_9      15149 non-null  int32\n",
      " 27  volume_1         15149 non-null  int32\n",
      " 28  volume_2         15149 non-null  int32\n",
      " 29  volume_3         15149 non-null  int32\n",
      " 30  volume_4         15149 non-null  int32\n",
      " 31  volume_5         15149 non-null  int32\n",
      " 32  volume_6         15149 non-null  int32\n",
      " 33  volume_7         15149 non-null  int32\n",
      " 34  volume_8         15149 non-null  int32\n",
      " 35  volume_9         15149 non-null  int32\n",
      " 36  closing_price_1  15149 non-null  int32\n",
      " 37  closing_price_2  15149 non-null  int32\n",
      " 38  closing_price_3  15149 non-null  int32\n",
      " 39  closing_price_4  15149 non-null  int32\n",
      " 40  closing_price_5  15149 non-null  int32\n",
      " 41  closing_price_6  15149 non-null  int32\n",
      " 42  closing_price_7  15149 non-null  int32\n",
      " 43  closing_price_8  15149 non-null  int32\n",
      " 44  closing_price_9  15149 non-null  int32\n",
      "dtypes: int32(45)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "rbscaler = RobustScaler().fit(X_train)\n",
    "# with open('robust_scaler.pkl', 'wb') as f:\n",
    "#     pickle.dump(rbscaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16751592, -0.18025478, -0.17620865, ..., -0.20140217,\n",
       "        -0.20700637, -0.21551176],\n",
       "       [-0.08598726, -0.07070064, -0.07315522, ..., -0.05035054,\n",
       "        -0.05031847, -0.05594406],\n",
       "       [ 0.42229299,  0.43503185,  0.40203562, ...,  0.61440408,\n",
       "         0.65159236,  0.60267006],\n",
       "       ...,\n",
       "       [-0.27388535, -0.26369427, -0.24681934, ..., -0.23773104,\n",
       "        -0.23312102, -0.23521933],\n",
       "       [-0.41592357, -0.41210191, -0.3975827 , ..., -0.41618866,\n",
       "        -0.39745223, -0.39542276],\n",
       "       [-0.62101911, -0.64012739, -0.67366412, ..., -0.70427024,\n",
       "        -0.71273885, -0.70438652]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = rbscaler.transform(X_train)\n",
    "X_val_scaled = rbscaler.transform(X_val)\n",
    "X_test_scaled = rbscaler.transform(X_test)\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_price_1</th>\n",
       "      <th>market_price_2</th>\n",
       "      <th>market_price_3</th>\n",
       "      <th>market_price_4</th>\n",
       "      <th>market_price_5</th>\n",
       "      <th>market_price_6</th>\n",
       "      <th>market_price_7</th>\n",
       "      <th>market_price_8</th>\n",
       "      <th>market_price_9</th>\n",
       "      <th>high_price_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_9</th>\n",
       "      <th>closing_price_1</th>\n",
       "      <th>closing_price_2</th>\n",
       "      <th>closing_price_3</th>\n",
       "      <th>closing_price_4</th>\n",
       "      <th>closing_price_5</th>\n",
       "      <th>closing_price_6</th>\n",
       "      <th>closing_price_7</th>\n",
       "      <th>closing_price_8</th>\n",
       "      <th>closing_price_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.167516</td>\n",
       "      <td>-0.180255</td>\n",
       "      <td>-0.176209</td>\n",
       "      <td>-0.171429</td>\n",
       "      <td>-0.173858</td>\n",
       "      <td>-0.179341</td>\n",
       "      <td>-0.188094</td>\n",
       "      <td>-0.194937</td>\n",
       "      <td>-0.201643</td>\n",
       "      <td>-0.174189</td>\n",
       "      <td>...</td>\n",
       "      <td>16.126548</td>\n",
       "      <td>-0.176395</td>\n",
       "      <td>-0.170935</td>\n",
       "      <td>-0.173385</td>\n",
       "      <td>-0.179553</td>\n",
       "      <td>-0.179962</td>\n",
       "      <td>-0.196554</td>\n",
       "      <td>-0.201402</td>\n",
       "      <td>-0.207006</td>\n",
       "      <td>-0.215512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.085987</td>\n",
       "      <td>-0.070701</td>\n",
       "      <td>-0.073155</td>\n",
       "      <td>-0.073651</td>\n",
       "      <td>-0.063452</td>\n",
       "      <td>-0.050063</td>\n",
       "      <td>-0.048765</td>\n",
       "      <td>-0.047468</td>\n",
       "      <td>-0.051201</td>\n",
       "      <td>-0.078830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133746</td>\n",
       "      <td>-0.065427</td>\n",
       "      <td>-0.071063</td>\n",
       "      <td>-0.075496</td>\n",
       "      <td>-0.053035</td>\n",
       "      <td>-0.047224</td>\n",
       "      <td>-0.058711</td>\n",
       "      <td>-0.050351</td>\n",
       "      <td>-0.050318</td>\n",
       "      <td>-0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.422293</td>\n",
       "      <td>0.435032</td>\n",
       "      <td>0.402036</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.540609</td>\n",
       "      <td>0.622940</td>\n",
       "      <td>0.713743</td>\n",
       "      <td>0.642405</td>\n",
       "      <td>0.625790</td>\n",
       "      <td>0.427845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133847</td>\n",
       "      <td>0.388069</td>\n",
       "      <td>0.441741</td>\n",
       "      <td>0.506718</td>\n",
       "      <td>0.550160</td>\n",
       "      <td>0.707084</td>\n",
       "      <td>0.643906</td>\n",
       "      <td>0.614404</td>\n",
       "      <td>0.651592</td>\n",
       "      <td>0.602670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750318</td>\n",
       "      <td>0.755414</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.754442</td>\n",
       "      <td>0.745247</td>\n",
       "      <td>0.742875</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.716814</td>\n",
       "      <td>0.741894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064747</td>\n",
       "      <td>0.753047</td>\n",
       "      <td>0.750960</td>\n",
       "      <td>0.762636</td>\n",
       "      <td>0.755911</td>\n",
       "      <td>0.751755</td>\n",
       "      <td>0.729419</td>\n",
       "      <td>0.725303</td>\n",
       "      <td>0.719108</td>\n",
       "      <td>0.724094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.149045</td>\n",
       "      <td>0.149045</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.162540</td>\n",
       "      <td>0.156091</td>\n",
       "      <td>0.155260</td>\n",
       "      <td>0.145662</td>\n",
       "      <td>0.150633</td>\n",
       "      <td>0.161188</td>\n",
       "      <td>0.141767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133344</td>\n",
       "      <td>0.164208</td>\n",
       "      <td>0.165813</td>\n",
       "      <td>0.163788</td>\n",
       "      <td>0.159105</td>\n",
       "      <td>0.129547</td>\n",
       "      <td>0.124442</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>0.114013</td>\n",
       "      <td>0.118881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122694</th>\n",
       "      <td>-0.724841</td>\n",
       "      <td>-0.717197</td>\n",
       "      <td>-0.724555</td>\n",
       "      <td>-0.723175</td>\n",
       "      <td>-0.725888</td>\n",
       "      <td>-0.715463</td>\n",
       "      <td>-0.715643</td>\n",
       "      <td>-0.713924</td>\n",
       "      <td>-0.714918</td>\n",
       "      <td>-0.721551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>-0.731238</td>\n",
       "      <td>-0.734315</td>\n",
       "      <td>-0.733845</td>\n",
       "      <td>-0.722045</td>\n",
       "      <td>-0.714103</td>\n",
       "      <td>-0.724952</td>\n",
       "      <td>-0.721479</td>\n",
       "      <td>-0.713376</td>\n",
       "      <td>-0.715194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122695</th>\n",
       "      <td>0.560510</td>\n",
       "      <td>0.549682</td>\n",
       "      <td>0.526081</td>\n",
       "      <td>0.530794</td>\n",
       "      <td>0.526650</td>\n",
       "      <td>0.527883</td>\n",
       "      <td>0.528182</td>\n",
       "      <td>0.510127</td>\n",
       "      <td>0.524020</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>0.554843</td>\n",
       "      <td>0.531370</td>\n",
       "      <td>0.536148</td>\n",
       "      <td>0.531629</td>\n",
       "      <td>0.532865</td>\n",
       "      <td>0.532865</td>\n",
       "      <td>0.514978</td>\n",
       "      <td>0.529299</td>\n",
       "      <td>0.528926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122696</th>\n",
       "      <td>-0.273885</td>\n",
       "      <td>-0.263694</td>\n",
       "      <td>-0.246819</td>\n",
       "      <td>-0.231746</td>\n",
       "      <td>-0.251904</td>\n",
       "      <td>-0.237643</td>\n",
       "      <td>-0.248892</td>\n",
       "      <td>-0.243671</td>\n",
       "      <td>-0.233249</td>\n",
       "      <td>-0.269549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>-0.241180</td>\n",
       "      <td>-0.250960</td>\n",
       "      <td>-0.252079</td>\n",
       "      <td>-0.254313</td>\n",
       "      <td>-0.248883</td>\n",
       "      <td>-0.240587</td>\n",
       "      <td>-0.237731</td>\n",
       "      <td>-0.233121</td>\n",
       "      <td>-0.235219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122697</th>\n",
       "      <td>-0.415924</td>\n",
       "      <td>-0.412102</td>\n",
       "      <td>-0.397583</td>\n",
       "      <td>-0.403810</td>\n",
       "      <td>-0.392766</td>\n",
       "      <td>-0.396071</td>\n",
       "      <td>-0.394554</td>\n",
       "      <td>-0.413291</td>\n",
       "      <td>-0.412769</td>\n",
       "      <td>-0.413859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127586</td>\n",
       "      <td>-0.401539</td>\n",
       "      <td>-0.382843</td>\n",
       "      <td>-0.392194</td>\n",
       "      <td>-0.404473</td>\n",
       "      <td>-0.389917</td>\n",
       "      <td>-0.412253</td>\n",
       "      <td>-0.416189</td>\n",
       "      <td>-0.397452</td>\n",
       "      <td>-0.395423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122698</th>\n",
       "      <td>-0.621019</td>\n",
       "      <td>-0.640127</td>\n",
       "      <td>-0.673664</td>\n",
       "      <td>-0.683175</td>\n",
       "      <td>-0.672589</td>\n",
       "      <td>-0.686312</td>\n",
       "      <td>-0.689677</td>\n",
       "      <td>-0.699367</td>\n",
       "      <td>-0.702908</td>\n",
       "      <td>-0.626828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129762</td>\n",
       "      <td>-0.663887</td>\n",
       "      <td>-0.679898</td>\n",
       "      <td>-0.690979</td>\n",
       "      <td>-0.678594</td>\n",
       "      <td>-0.691130</td>\n",
       "      <td>-0.692406</td>\n",
       "      <td>-0.704270</td>\n",
       "      <td>-0.712739</td>\n",
       "      <td>-0.704387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122699 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        market_price_1  market_price_2  market_price_3  market_price_4  \\\n",
       "0            -0.167516       -0.180255       -0.176209       -0.171429   \n",
       "1            -0.085987       -0.070701       -0.073155       -0.073651   \n",
       "2             0.422293        0.435032        0.402036        0.514286   \n",
       "3             0.750318        0.755414        0.744275        0.764444   \n",
       "4             0.149045        0.149045        0.164122        0.162540   \n",
       "...                ...             ...             ...             ...   \n",
       "122694       -0.724841       -0.717197       -0.724555       -0.723175   \n",
       "122695        0.560510        0.549682        0.526081        0.530794   \n",
       "122696       -0.273885       -0.263694       -0.246819       -0.231746   \n",
       "122697       -0.415924       -0.412102       -0.397583       -0.403810   \n",
       "122698       -0.621019       -0.640127       -0.673664       -0.683175   \n",
       "\n",
       "        market_price_5  market_price_6  market_price_7  market_price_8  \\\n",
       "0            -0.173858       -0.179341       -0.188094       -0.194937   \n",
       "1            -0.063452       -0.050063       -0.048765       -0.047468   \n",
       "2             0.540609        0.622940        0.713743        0.642405   \n",
       "3             0.754442        0.745247        0.742875        0.718354   \n",
       "4             0.156091        0.155260        0.145662        0.150633   \n",
       "...                ...             ...             ...             ...   \n",
       "122694       -0.725888       -0.715463       -0.715643       -0.713924   \n",
       "122695        0.526650        0.527883        0.528182        0.510127   \n",
       "122696       -0.251904       -0.237643       -0.248892       -0.243671   \n",
       "122697       -0.392766       -0.396071       -0.394554       -0.413291   \n",
       "122698       -0.672589       -0.686312       -0.689677       -0.699367   \n",
       "\n",
       "        market_price_9  high_price_1  ...   volume_9  closing_price_1  \\\n",
       "0            -0.201643     -0.174189  ...  16.126548        -0.176395   \n",
       "1            -0.051201     -0.078830  ...  -0.133746        -0.065427   \n",
       "2             0.625790      0.427845  ...  -0.133847         0.388069   \n",
       "3             0.716814      0.741894  ...  -0.064747         0.753047   \n",
       "4             0.161188      0.141767  ...  -0.133344         0.164208   \n",
       "...                ...           ...  ...        ...              ...   \n",
       "122694       -0.714918     -0.721551  ...   0.070506        -0.731238   \n",
       "122695        0.524020      0.552448  ...  -0.134248         0.554843   \n",
       "122696       -0.233249     -0.269549  ...   0.096552        -0.241180   \n",
       "122697       -0.412769     -0.413859  ...  -0.127586        -0.401539   \n",
       "122698       -0.702908     -0.626828  ...  -0.129762        -0.663887   \n",
       "\n",
       "        closing_price_2  closing_price_3  closing_price_4  closing_price_5  \\\n",
       "0             -0.170935        -0.173385        -0.179553        -0.179962   \n",
       "1             -0.071063        -0.075496        -0.053035        -0.047224   \n",
       "2              0.441741         0.506718         0.550160         0.707084   \n",
       "3              0.750960         0.762636         0.755911         0.751755   \n",
       "4              0.165813         0.163788         0.159105         0.129547   \n",
       "...                 ...              ...              ...              ...   \n",
       "122694        -0.734315        -0.733845        -0.722045        -0.714103   \n",
       "122695         0.531370         0.536148         0.531629         0.532865   \n",
       "122696        -0.250960        -0.252079        -0.254313        -0.248883   \n",
       "122697        -0.382843        -0.392194        -0.404473        -0.389917   \n",
       "122698        -0.679898        -0.690979        -0.678594        -0.691130   \n",
       "\n",
       "        closing_price_6  closing_price_7  closing_price_8  closing_price_9  \n",
       "0             -0.196554        -0.201402        -0.207006        -0.215512  \n",
       "1             -0.058711        -0.050351        -0.050318        -0.055944  \n",
       "2              0.643906         0.614404         0.651592         0.602670  \n",
       "3              0.729419         0.725303         0.719108         0.724094  \n",
       "4              0.124442         0.128107         0.114013         0.118881  \n",
       "...                 ...              ...              ...              ...  \n",
       "122694        -0.724952        -0.721479        -0.713376        -0.715194  \n",
       "122695         0.532865         0.514978         0.529299         0.528926  \n",
       "122696        -0.240587        -0.237731        -0.233121        -0.235219  \n",
       "122697        -0.412253        -0.416189        -0.397452        -0.395423  \n",
       "122698        -0.692406        -0.704270        -0.712739        -0.704387  \n",
       "\n",
       "[122699 rows x 45 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "X_val = pd.DataFrame(X_val_scaled, columns = X_val.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns = X_test.columns)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNModel(\n",
      "  (in_layer): Linear(in_features=45, out_features=600, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (hd_layers): ModuleList(\n",
      "    (0): Linear(in_features=600, out_features=550, bias=True)\n",
      "    (1): Dropout(p=0.25, inplace=False)\n",
      "    (2): Linear(in_features=550, out_features=500, bias=True)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=450, bias=True)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Linear(in_features=450, out_features=400, bias=True)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=400, out_features=350, bias=True)\n",
      "    (9): Dropout(p=0.25, inplace=False)\n",
      "    (10): Linear(in_features=350, out_features=300, bias=True)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Linear(in_features=300, out_features=250, bias=True)\n",
      "    (13): Dropout(p=0.25, inplace=False)\n",
      "    (14): Linear(in_features=250, out_features=200, bias=True)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "    (16): Linear(in_features=200, out_features=150, bias=True)\n",
      "    (17): Dropout(p=0.25, inplace=False)\n",
      "    (18): Linear(in_features=150, out_features=100, bias=True)\n",
      "    (19): Dropout(p=0.25, inplace=False)\n",
      "    (20): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (21): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (out_layer): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "h_inouts = range(600, 40, -50)\n",
    "dropout_prob = 0.25\n",
    "\n",
    "ann_model = ANNModel(in_in = 45, in_out = 100, out_out = 1, h_ins = h_inouts, h_outs = h_inouts, dropout_prob = dropout_prob)\n",
    "print(ann_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS = CustomDataset(X_train, y_train)\n",
    "\n",
    "trainDL = DataLoader(trainDS, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(ann_model.parameters(), lr = LR)\n",
    "regLoss = nn.MSELoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(featureDF, targetDF, model):\n",
    "    featureTS = torch.FloatTensor(featureDF.values).to(DEVICE)\n",
    "    targetTS = torch.FloatTensor(targetDF.values).to(DEVICE)\n",
    "    model.dropout = nn.Dropout(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pre_val = model(featureTS)\n",
    "        loss_val = regLoss(pre_val, targetTS)\n",
    "        score_val = R2Score()(pre_val, targetTS)\n",
    "    \n",
    "    return loss_val, score_val, pre_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = './saved_models/'\n",
    "os.makedirs(SAVE_PATH, exist_ok = True)\n",
    "\n",
    "def training(valDF, valtargetDF, model):\n",
    "\n",
    "    BREAK_CNT_LOSS = 0\n",
    "    BREAK_CNT_SCORE = 0\n",
    "    LIMIT_VALUE = 10\n",
    "\n",
    "    LOSS_HISTORY, SCORE_HISTORY = [[], []], [[], []]\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        SAVE_MODEL = os.path.join(SAVE_PATH, f'model_{epoch}.pth')\n",
    "        SAVE_WEIGHT = os.path.join(SAVE_PATH, f'model_weights_{epoch}.pth')\n",
    "\n",
    "        loss_total, score_total = 0, 0\n",
    "\n",
    "        for featureTS, targetTS in trainDL:\n",
    "            \n",
    "            pre_y = model(featureTS)\n",
    "\n",
    "            loss = regLoss(pre_y, targetTS)\n",
    "            loss_total += loss.item()\n",
    "\n",
    "            score = R2Score()(pre_y, targetTS)\n",
    "            score_total += score.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_r2, pre_val = testing(valDF, valtargetDF, model)\n",
    "\n",
    "        LOSS_HISTORY[1].append(val_loss)\n",
    "        SCORE_HISTORY[1].append(val_r2)\n",
    "\n",
    "        LOSS_HISTORY[0].append(loss_total / len(trainDL))\n",
    "        SCORE_HISTORY[0].append(score_total / len(trainDL))\n",
    "        print(f\"pre_val : {pre_val}\\n y_val:{y_val}\")\n",
    "        print(f\"[{epoch + 1}/{EPOCH}]\\n - TRAIN LOSS : {LOSS_HISTORY[0][-1]} R2 : {SCORE_HISTORY[0][-1]}\")\n",
    "        print(f\"- VAL LOSS : {LOSS_HISTORY[1][-1]} R2 : {SCORE_HISTORY[1][-1]}\")\n",
    "\n",
    "        scheduler.step(val_r2)\n",
    "\n",
    "        if len(LOSS_HISTORY[1]) >= 2:\n",
    "            if LOSS_HISTORY[1][-1] >= LOSS_HISTORY[1][-2]: BREAK_CNT_LOSS += 1\n",
    "        \n",
    "        if len(LOSS_HISTORY[1]) == 1:\n",
    "            torch.save(model.state_dict(), SAVE_WEIGHT)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "        \n",
    "        else:\n",
    "            if LOSS_HISTORY[1][-1] < min(LOSS_HISTORY[1][:-1]):\n",
    "                torch.save(model.state_dict(), SAVE_WEIGHT)\n",
    "                torch.save(model, SAVE_MODEL)\n",
    "\n",
    "        if BREAK_CNT_LOSS > LIMIT_VALUE:\n",
    "            print(f\"성능 및 손실 개선이 없어서 {epoch} EPOCH에 학습 중단\")\n",
    "\n",
    "    \n",
    "    return LOSS_HISTORY, SCORE_HISTORY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_val : tensor([[10284.9209],\n",
      "        [13856.9229],\n",
      "        [10388.7773],\n",
      "        ...,\n",
      "        [16023.9258],\n",
      "        [ 7035.9819],\n",
      "        [10936.5898]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[1/1000]\n",
      " - TRAIN LOSS : 143517194.85267276 R2 : 0.7623834089785232\n",
      "- VAL LOSS : 11346652.0 R2 : 0.9843507409095764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mann_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 30\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(valDF, valtargetDF, model)\u001b[0m\n\u001b[0;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     29\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 30\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m val_loss, val_r2, pre_val \u001b[38;5;241m=\u001b[39m testing(valDF, valtargetDF, model)\n\u001b[0;32m     34\u001b[0m LOSS_HISTORY[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_loss)\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         state_steps,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\adam.py:433\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 433\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, r2 = training(X_val, y_val, ann_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_size, n_layers, dropout,\n",
    "                 bidirectional, model_type):\n",
    "        super().__init__()\n",
    "\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = input_size,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout = dropout,\n",
    "                batch_first = True\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        else:\n",
    "            self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output, _ = self.model(inputs)\n",
    "        last_output = self.dropout(output)\n",
    "        logits = self.linear(last_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 45\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(input_size = input_size, hidden_dim = hidden_dim,\n",
    "                      n_layers = n_layers, dropout = 0.9, bidirectional = False,\n",
    "                      model_type = 'lstm').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1000\n",
    "LR = 0.001\n",
    "\n",
    "regLoss = nn.L1Loss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr = LR)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 100, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_val : tensor([[10405.4727],\n",
      "        [14237.4932],\n",
      "        [10508.9609],\n",
      "        ...,\n",
      "        [15874.3750],\n",
      "        [ 6963.7900],\n",
      "        [11385.8975]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[1/1000]\n",
      " - TRAIN LOSS : 308.01367537039664 R2 : 0.9980932957035\n",
      "- VAL LOSS : 268.5990295410156 R2 : 0.9995244145393372\n",
      "pre_val : tensor([[10550.0273],\n",
      "        [14496.3496],\n",
      "        [10561.9844],\n",
      "        ...,\n",
      "        [15822.9561],\n",
      "        [ 6843.7690],\n",
      "        [11361.7314]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[2/1000]\n",
      " - TRAIN LOSS : 316.7618456028586 R2 : 0.9980818732310149\n",
      "- VAL LOSS : 258.7540283203125 R2 : 0.9995638728141785\n",
      "pre_val : tensor([[10537.7988],\n",
      "        [14503.7725],\n",
      "        [10503.2646],\n",
      "        ...,\n",
      "        [15770.4033],\n",
      "        [ 6872.3271],\n",
      "        [11284.0049]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[3/1000]\n",
      " - TRAIN LOSS : 312.61714163609867 R2 : 0.9981670337829938\n",
      "- VAL LOSS : 270.0343017578125 R2 : 0.9995718002319336\n",
      "pre_val : tensor([[10502.2920],\n",
      "        [14389.7354],\n",
      "        [10512.9180],\n",
      "        ...,\n",
      "        [15786.4980],\n",
      "        [ 6788.9097],\n",
      "        [11331.4395]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[4/1000]\n",
      " - TRAIN LOSS : 310.1394973138165 R2 : 0.998220826050138\n",
      "- VAL LOSS : 275.9329833984375 R2 : 0.9995818138122559\n",
      "pre_val : tensor([[10476.5869],\n",
      "        [14457.1768],\n",
      "        [10627.1846],\n",
      "        ...,\n",
      "        [15910.2061],\n",
      "        [ 6807.6533],\n",
      "        [11361.0479]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[5/1000]\n",
      " - TRAIN LOSS : 310.876098746209 R2 : 0.9981742401166566\n",
      "- VAL LOSS : 264.30950927734375 R2 : 0.9995377659797668\n",
      "pre_val : tensor([[10508.7607],\n",
      "        [14366.3057],\n",
      "        [10582.3037],\n",
      "        ...,\n",
      "        [15776.3213],\n",
      "        [ 6858.9492],\n",
      "        [11343.6758]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[6/1000]\n",
      " - TRAIN LOSS : 315.72305844939825 R2 : 0.9979646831945148\n",
      "- VAL LOSS : 256.0575866699219 R2 : 0.9995989203453064\n",
      "pre_val : tensor([[10518.1689],\n",
      "        [14365.5811],\n",
      "        [10499.5869],\n",
      "        ...,\n",
      "        [15796.4756],\n",
      "        [ 6887.7769],\n",
      "        [11339.9297]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[7/1000]\n",
      " - TRAIN LOSS : 313.02994453201393 R2 : 0.9982538598768093\n",
      "- VAL LOSS : 251.5946044921875 R2 : 0.9996278882026672\n",
      "pre_val : tensor([[10515.6387],\n",
      "        [14248.3057],\n",
      "        [10407.7959],\n",
      "        ...,\n",
      "        [15859.4775],\n",
      "        [ 6840.6499],\n",
      "        [11237.4414]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[8/1000]\n",
      " - TRAIN LOSS : 310.9167836056228 R2 : 0.9982294228117344\n",
      "- VAL LOSS : 272.1009521484375 R2 : 0.9995090961456299\n",
      "pre_val : tensor([[10457.9980],\n",
      "        [14267.5566],\n",
      "        [10585.0527],\n",
      "        ...,\n",
      "        [15847.6680],\n",
      "        [ 6968.9004],\n",
      "        [11356.4590]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[9/1000]\n",
      " - TRAIN LOSS : 310.55573019254007 R2 : 0.9982199268981407\n",
      "- VAL LOSS : 262.142333984375 R2 : 0.9995940923690796\n",
      "pre_val : tensor([[10451.4102],\n",
      "        [14424.1807],\n",
      "        [10559.5820],\n",
      "        ...,\n",
      "        [15868.9688],\n",
      "        [ 6910.0420],\n",
      "        [11391.1445]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[10/1000]\n",
      " - TRAIN LOSS : 310.98442229627784 R2 : 0.9982609687914593\n",
      "- VAL LOSS : 263.6109313964844 R2 : 0.9996014833450317\n",
      "pre_val : tensor([[10495.2344],\n",
      "        [14384.9795],\n",
      "        [10611.6953],\n",
      "        ...,\n",
      "        [15893.2588],\n",
      "        [ 6922.4478],\n",
      "        [11349.6982]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[11/1000]\n",
      " - TRAIN LOSS : 309.37953653348006 R2 : 0.9982015275737608\n",
      "- VAL LOSS : 250.11074829101562 R2 : 0.9996252655982971\n",
      "pre_val : tensor([[10492.4912],\n",
      "        [14427.7959],\n",
      "        [10597.3750],\n",
      "        ...,\n",
      "        [15904.1504],\n",
      "        [ 6930.6460],\n",
      "        [11336.3623]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[12/1000]\n",
      " - TRAIN LOSS : 310.65858402177594 R2 : 0.9982705211390728\n",
      "- VAL LOSS : 251.96617126464844 R2 : 0.9995872974395752\n",
      "pre_val : tensor([[10558.6445],\n",
      "        [14641.3740],\n",
      "        [10582.5215],\n",
      "        ...,\n",
      "        [15865.4375],\n",
      "        [ 6922.2563],\n",
      "        [11332.0645]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[13/1000]\n",
      " - TRAIN LOSS : 305.883349271175 R2 : 0.998349217998624\n",
      "- VAL LOSS : 264.3783264160156 R2 : 0.9995758533477783\n",
      "pre_val : tensor([[10442.9902],\n",
      "        [14280.4551],\n",
      "        [10502.6543],\n",
      "        ...,\n",
      "        [15983.5635],\n",
      "        [ 6828.8804],\n",
      "        [11301.8037]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[14/1000]\n",
      " - TRAIN LOSS : 306.7924164753383 R2 : 0.9983559911067669\n",
      "- VAL LOSS : 262.1563415527344 R2 : 0.9995672106742859\n",
      "pre_val : tensor([[10465.0166],\n",
      "        [14332.5947],\n",
      "        [10519.8076],\n",
      "        ...,\n",
      "        [15815.7520],\n",
      "        [ 6914.0547],\n",
      "        [11319.3369]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[15/1000]\n",
      " - TRAIN LOSS : 306.6085796928157 R2 : 0.9982861927777257\n",
      "- VAL LOSS : 251.4365234375 R2 : 0.9996380805969238\n",
      "pre_val : tensor([[10496.3486],\n",
      "        [14315.6895],\n",
      "        [10553.3311],\n",
      "        ...,\n",
      "        [15822.8174],\n",
      "        [ 6886.6699],\n",
      "        [11299.1758]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[16/1000]\n",
      " - TRAIN LOSS : 303.7452709949001 R2 : 0.998379054218725\n",
      "- VAL LOSS : 265.04998779296875 R2 : 0.9995629191398621\n",
      "pre_val : tensor([[10483.5049],\n",
      "        [14386.4785],\n",
      "        [10545.0957],\n",
      "        ...,\n",
      "        [15868.8350],\n",
      "        [ 6861.2310],\n",
      "        [11441.3281]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[17/1000]\n",
      " - TRAIN LOSS : 302.73056973652547 R2 : 0.9983678453127128\n",
      "- VAL LOSS : 261.79217529296875 R2 : 0.9996129274368286\n",
      "pre_val : tensor([[10541.6006],\n",
      "        [14301.8916],\n",
      "        [10522.6924],\n",
      "        ...,\n",
      "        [15870.3477],\n",
      "        [ 6795.4722],\n",
      "        [11355.0801]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[18/1000]\n",
      " - TRAIN LOSS : 301.784160782119 R2 : 0.9983743843541152\n",
      "- VAL LOSS : 260.6585998535156 R2 : 0.9996033906936646\n",
      "pre_val : tensor([[10536.8428],\n",
      "        [14236.1650],\n",
      "        [10536.7998],\n",
      "        ...,\n",
      "        [15815.2920],\n",
      "        [ 6899.5366],\n",
      "        [11347.3096]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[19/1000]\n",
      " - TRAIN LOSS : 304.20589381768775 R2 : 0.9983698593124728\n",
      "- VAL LOSS : 250.50454711914062 R2 : 0.999618649482727\n",
      "pre_val : tensor([[10565.6328],\n",
      "        [14393.2949],\n",
      "        [10579.4121],\n",
      "        ...,\n",
      "        [15793.0723],\n",
      "        [ 6891.9033],\n",
      "        [11371.1885]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[20/1000]\n",
      " - TRAIN LOSS : 308.2905370998258 R2 : 0.9983189523453196\n",
      "- VAL LOSS : 249.29544067382812 R2 : 0.9996501207351685\n",
      "pre_val : tensor([[10509.2432],\n",
      "        [14205.2490],\n",
      "        [10561.9473],\n",
      "        ...,\n",
      "        [15802.4492],\n",
      "        [ 6879.2373],\n",
      "        [11279.2207]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[21/1000]\n",
      " - TRAIN LOSS : 308.22301693634074 R2 : 0.9983744643657739\n",
      "- VAL LOSS : 249.74684143066406 R2 : 0.9996448159217834\n",
      "pre_val : tensor([[10560.1426],\n",
      "        [14199.0840],\n",
      "        [10472.5996],\n",
      "        ...,\n",
      "        [15841.0352],\n",
      "        [ 6749.3867],\n",
      "        [11252.9902]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[22/1000]\n",
      " - TRAIN LOSS : 304.0838564458695 R2 : 0.9983880080797526\n",
      "- VAL LOSS : 270.37054443359375 R2 : 0.9995803833007812\n",
      "pre_val : tensor([[10546.7598],\n",
      "        [14457.9863],\n",
      "        [10546.8682],\n",
      "        ...,\n",
      "        [15863.9297],\n",
      "        [ 6812.0542],\n",
      "        [11271.7783]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[23/1000]\n",
      " - TRAIN LOSS : 307.28109549630574 R2 : 0.9982980331821267\n",
      "- VAL LOSS : 256.8543395996094 R2 : 0.9996169805526733\n",
      "pre_val : tensor([[10554.7061],\n",
      "        [14293.6504],\n",
      "        [10546.7607],\n",
      "        ...,\n",
      "        [15829.8916],\n",
      "        [ 6930.8828],\n",
      "        [11331.2725]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[24/1000]\n",
      " - TRAIN LOSS : 302.82476099603645 R2 : 0.9983930315156959\n",
      "- VAL LOSS : 252.8096923828125 R2 : 0.9996530413627625\n",
      "pre_val : tensor([[10511.0195],\n",
      "        [14450.0117],\n",
      "        [10551.8438],\n",
      "        ...,\n",
      "        [15828.9873],\n",
      "        [ 6779.3711],\n",
      "        [11351.4033]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[25/1000]\n",
      " - TRAIN LOSS : 300.4720237741881 R2 : 0.9984456502629694\n",
      "- VAL LOSS : 254.56210327148438 R2 : 0.999578595161438\n",
      "pre_val : tensor([[10499.8945],\n",
      "        [14314.6904],\n",
      "        [10558.5586],\n",
      "        ...,\n",
      "        [15871.3965],\n",
      "        [ 6981.8496],\n",
      "        [11314.4814]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[26/1000]\n",
      " - TRAIN LOSS : 299.3524079204694 R2 : 0.9984783997747081\n",
      "- VAL LOSS : 244.30380249023438 R2 : 0.9996748566627502\n",
      "pre_val : tensor([[10531.3701],\n",
      "        [14379.1914],\n",
      "        [10643.1748],\n",
      "        ...,\n",
      "        [15950.0654],\n",
      "        [ 6803.4580],\n",
      "        [11266.9746]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[27/1000]\n",
      " - TRAIN LOSS : 299.16277450899724 R2 : 0.9985054144026247\n",
      "- VAL LOSS : 272.1459045410156 R2 : 0.9995654225349426\n",
      "성능 및 손실 개선이 없어서 26 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10522.1914],\n",
      "        [14378.7344],\n",
      "        [10629.5410],\n",
      "        ...,\n",
      "        [15765.2041],\n",
      "        [ 6884.6147],\n",
      "        [11350.3281]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[28/1000]\n",
      " - TRAIN LOSS : 301.3062567122757 R2 : 0.9983605094652288\n",
      "- VAL LOSS : 252.4725341796875 R2 : 0.9996519088745117\n",
      "성능 및 손실 개선이 없어서 27 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10532.8203],\n",
      "        [14266.4199],\n",
      "        [10646.7549],\n",
      "        ...,\n",
      "        [15878.0439],\n",
      "        [ 6884.2422],\n",
      "        [11244.6816]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[29/1000]\n",
      " - TRAIN LOSS : 301.9748783872522 R2 : 0.9984204277221351\n",
      "- VAL LOSS : 258.2950439453125 R2 : 0.9995949864387512\n",
      "성능 및 손실 개선이 없어서 28 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10492.7686],\n",
      "        [14202.1221],\n",
      "        [10570.6309],\n",
      "        ...,\n",
      "        [15830.0635],\n",
      "        [ 6887.9932],\n",
      "        [11319.4971]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[30/1000]\n",
      " - TRAIN LOSS : 299.6085878324944 R2 : 0.9984377196590468\n",
      "- VAL LOSS : 253.1005096435547 R2 : 0.9996240139007568\n",
      "성능 및 손실 개선이 없어서 29 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10465.3652],\n",
      "        [14350.6758],\n",
      "        [10603.5635],\n",
      "        ...,\n",
      "        [15943.2725],\n",
      "        [ 6893.3789],\n",
      "        [11382.7207]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[31/1000]\n",
      " - TRAIN LOSS : 298.52879359358445 R2 : 0.9985301238772456\n",
      "- VAL LOSS : 254.3627166748047 R2 : 0.9996116161346436\n",
      "성능 및 손실 개선이 없어서 30 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10497.7998],\n",
      "        [14498.0166],\n",
      "        [10568.5996],\n",
      "        ...,\n",
      "        [15876.9033],\n",
      "        [ 6850.5835],\n",
      "        [11327.9912]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[32/1000]\n",
      " - TRAIN LOSS : 298.072403429571 R2 : 0.9984989224387893\n",
      "- VAL LOSS : 246.4050750732422 R2 : 0.9996331930160522\n",
      "성능 및 손실 개선이 없어서 31 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10510.7646],\n",
      "        [14401.7471],\n",
      "        [10596.6221],\n",
      "        ...,\n",
      "        [15868.9160],\n",
      "        [ 6941.5332],\n",
      "        [11345.7939]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[33/1000]\n",
      " - TRAIN LOSS : 301.8270754300631 R2 : 0.9983693634826482\n",
      "- VAL LOSS : 258.4670715332031 R2 : 0.9995993375778198\n",
      "성능 및 손실 개선이 없어서 32 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10439.6787],\n",
      "        [14165.2822],\n",
      "        [10608.5322],\n",
      "        ...,\n",
      "        [15801.3936],\n",
      "        [ 6897.0107],\n",
      "        [11256.2080]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[34/1000]\n",
      " - TRAIN LOSS : 302.3053917338982 R2 : 0.9984150729092346\n",
      "- VAL LOSS : 249.93260192871094 R2 : 0.9996271133422852\n",
      "성능 및 손실 개선이 없어서 33 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10467.6631],\n",
      "        [14483.6123],\n",
      "        [10567.0889],\n",
      "        ...,\n",
      "        [15873.9502],\n",
      "        [ 6911.6851],\n",
      "        [11316.6299]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[35/1000]\n",
      " - TRAIN LOSS : 298.73315192549603 R2 : 0.9985269249962082\n",
      "- VAL LOSS : 266.3778076171875 R2 : 0.9995414614677429\n",
      "성능 및 손실 개선이 없어서 34 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10515.6328],\n",
      "        [14315.2412],\n",
      "        [10603.8008],\n",
      "        ...,\n",
      "        [15765.7461],\n",
      "        [ 6902.5762],\n",
      "        [11316.1963]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[36/1000]\n",
      " - TRAIN LOSS : 300.23823309827503 R2 : 0.9983804861134625\n",
      "- VAL LOSS : 261.9325256347656 R2 : 0.9995986223220825\n",
      "성능 및 손실 개선이 없어서 35 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10453.9697],\n",
      "        [14338.0898],\n",
      "        [10536.6914],\n",
      "        ...,\n",
      "        [15869.8594],\n",
      "        [ 6939.5874],\n",
      "        [11281.1904]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[37/1000]\n",
      " - TRAIN LOSS : 295.74491013249644 R2 : 0.9985800598404398\n",
      "- VAL LOSS : 250.3197021484375 R2 : 0.9996413588523865\n",
      "성능 및 손실 개선이 없어서 36 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10452.7441],\n",
      "        [14169.6680],\n",
      "        [10603.3760],\n",
      "        ...,\n",
      "        [15859.6553],\n",
      "        [ 6872.2007],\n",
      "        [11327.9238]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[38/1000]\n",
      " - TRAIN LOSS : 294.5994501504314 R2 : 0.9985842076087713\n",
      "- VAL LOSS : 256.7927551269531 R2 : 0.9996408820152283\n",
      "성능 및 손실 개선이 없어서 37 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10497.8584],\n",
      "        [14448.0898],\n",
      "        [10585.9219],\n",
      "        ...,\n",
      "        [15874.2871],\n",
      "        [ 6848.7915],\n",
      "        [11306.6074]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[39/1000]\n",
      " - TRAIN LOSS : 295.27862039538684 R2 : 0.998575517251563\n",
      "- VAL LOSS : 247.6842498779297 R2 : 0.9996556043624878\n",
      "성능 및 손실 개선이 없어서 38 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10447.8174],\n",
      "        [14261.1318],\n",
      "        [10556.6816],\n",
      "        ...,\n",
      "        [15762.9648],\n",
      "        [ 6880.2754],\n",
      "        [11309.8916]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[40/1000]\n",
      " - TRAIN LOSS : 295.30613061937237 R2 : 0.998528024767959\n",
      "- VAL LOSS : 251.8224639892578 R2 : 0.9996452927589417\n",
      "성능 및 손실 개선이 없어서 39 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10469.1562],\n",
      "        [14388.5820],\n",
      "        [10631.0303],\n",
      "        ...,\n",
      "        [15851.4619],\n",
      "        [ 6903.1597],\n",
      "        [11328.7539]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[41/1000]\n",
      " - TRAIN LOSS : 294.92375519316073 R2 : 0.9985879643488739\n",
      "- VAL LOSS : 245.0278778076172 R2 : 0.9996594190597534\n",
      "성능 및 손실 개선이 없어서 40 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10423.9668],\n",
      "        [14446.5684],\n",
      "        [10511.4014],\n",
      "        ...,\n",
      "        [15858.2363],\n",
      "        [ 6900.2344],\n",
      "        [11358.3447]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[42/1000]\n",
      " - TRAIN LOSS : 297.43551379505936 R2 : 0.998522318279883\n",
      "- VAL LOSS : 256.54833984375 R2 : 0.9996350407600403\n",
      "성능 및 손실 개선이 없어서 41 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10477.1816],\n",
      "        [14439.6631],\n",
      "        [10534.4521],\n",
      "        ...,\n",
      "        [15876.7900],\n",
      "        [ 6857.7183],\n",
      "        [11274.6904]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[43/1000]\n",
      " - TRAIN LOSS : 295.96382159216927 R2 : 0.9985474550894995\n",
      "- VAL LOSS : 249.2252655029297 R2 : 0.9996452331542969\n",
      "성능 및 손실 개선이 없어서 42 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10526.9414],\n",
      "        [14204.7178],\n",
      "        [10579.1709],\n",
      "        ...,\n",
      "        [15890.3965],\n",
      "        [ 6866.7480],\n",
      "        [11307.1299]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[44/1000]\n",
      " - TRAIN LOSS : 294.28355570050854 R2 : 0.99855635853137\n",
      "- VAL LOSS : 250.07542419433594 R2 : 0.9996069669723511\n",
      "성능 및 손실 개선이 없어서 43 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10492.1523],\n",
      "        [14194.6895],\n",
      "        [10599.4756],\n",
      "        ...,\n",
      "        [15793.2139],\n",
      "        [ 6966.8906],\n",
      "        [11290.2676]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[45/1000]\n",
      " - TRAIN LOSS : 298.47191697062226 R2 : 0.9985112986633022\n",
      "- VAL LOSS : 264.5667724609375 R2 : 0.9995800852775574\n",
      "성능 및 손실 개선이 없어서 44 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10550.8086],\n",
      "        [14290.4375],\n",
      "        [10595.0312],\n",
      "        ...,\n",
      "        [15788.7588],\n",
      "        [ 6920.1738],\n",
      "        [11338.5645]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[46/1000]\n",
      " - TRAIN LOSS : 295.3685263204761 R2 : 0.99861868518894\n",
      "- VAL LOSS : 259.633544921875 R2 : 0.9996026158332825\n",
      "성능 및 손실 개선이 없어서 45 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10492.0977],\n",
      "        [14199.0039],\n",
      "        [10587.9248],\n",
      "        ...,\n",
      "        [15872.1338],\n",
      "        [ 7080.1079],\n",
      "        [11422.1641]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[47/1000]\n",
      " - TRAIN LOSS : 291.98133834381304 R2 : 0.9986366811911472\n",
      "- VAL LOSS : 250.0008544921875 R2 : 0.999650776386261\n",
      "성능 및 손실 개선이 없어서 46 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10559.2461],\n",
      "        [14334.8652],\n",
      "        [10553.2959],\n",
      "        ...,\n",
      "        [15914.8398],\n",
      "        [ 6865.7803],\n",
      "        [11393.6562]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[48/1000]\n",
      " - TRAIN LOSS : 290.31541297047204 R2 : 0.998654147932085\n",
      "- VAL LOSS : 244.11209106445312 R2 : 0.9996591806411743\n",
      "성능 및 손실 개선이 없어서 47 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10604.0918],\n",
      "        [14376.9746],\n",
      "        [10585.4609],\n",
      "        ...,\n",
      "        [15779.1348],\n",
      "        [ 6937.0894],\n",
      "        [11327.2324]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[49/1000]\n",
      " - TRAIN LOSS : 290.15161701585356 R2 : 0.9987016927310043\n",
      "- VAL LOSS : 246.80955505371094 R2 : 0.9996499419212341\n",
      "성능 및 손실 개선이 없어서 48 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10474.6104],\n",
      "        [14309.7227],\n",
      "        [10579.7451],\n",
      "        ...,\n",
      "        [15850.0273],\n",
      "        [ 6884.6714],\n",
      "        [11386.3271]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[50/1000]\n",
      " - TRAIN LOSS : 289.46298946693946 R2 : 0.9986168797976657\n",
      "- VAL LOSS : 248.11056518554688 R2 : 0.9996611475944519\n",
      "성능 및 손실 개선이 없어서 49 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10546.8174],\n",
      "        [14479.9053],\n",
      "        [10584.9727],\n",
      "        ...,\n",
      "        [15852.9248],\n",
      "        [ 6841.0474],\n",
      "        [11308.4180]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[51/1000]\n",
      " - TRAIN LOSS : 289.684516654083 R2 : 0.9986556402241236\n",
      "- VAL LOSS : 244.3231201171875 R2 : 0.9996522068977356\n",
      "성능 및 손실 개선이 없어서 50 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10464.8398],\n",
      "        [14477.0596],\n",
      "        [10602.6846],\n",
      "        ...,\n",
      "        [15924.9434],\n",
      "        [ 6941.2900],\n",
      "        [11336.1953]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[52/1000]\n",
      " - TRAIN LOSS : 286.71779014458093 R2 : 0.9987124656138862\n",
      "- VAL LOSS : 247.6601104736328 R2 : 0.9996366500854492\n",
      "성능 및 손실 개선이 없어서 51 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10456.5869],\n",
      "        [14216.1504],\n",
      "        [10567.9268],\n",
      "        ...,\n",
      "        [15908.9268],\n",
      "        [ 6906.2764],\n",
      "        [11314.0957]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[53/1000]\n",
      " - TRAIN LOSS : 287.7118875993433 R2 : 0.9986887141187933\n",
      "- VAL LOSS : 239.51527404785156 R2 : 0.999688446521759\n",
      "성능 및 손실 개선이 없어서 52 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10570.3184],\n",
      "        [14280.8662],\n",
      "        [10592.6240],\n",
      "        ...,\n",
      "        [15942.9561],\n",
      "        [ 6944.1328],\n",
      "        [11351.6113]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[54/1000]\n",
      " - TRAIN LOSS : 286.62023652833943 R2 : 0.998728370169151\n",
      "- VAL LOSS : 252.23074340820312 R2 : 0.9996604323387146\n",
      "성능 및 손실 개선이 없어서 53 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10526.6631],\n",
      "        [14443.4482],\n",
      "        [10623.9141],\n",
      "        ...,\n",
      "        [15823.4111],\n",
      "        [ 6938.2983],\n",
      "        [11394.6982]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[55/1000]\n",
      " - TRAIN LOSS : 294.05372036134406 R2 : 0.9986360277159739\n",
      "- VAL LOSS : 260.98834228515625 R2 : 0.9996165633201599\n",
      "성능 및 손실 개선이 없어서 54 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10535.3867],\n",
      "        [14451.2842],\n",
      "        [10519.6602],\n",
      "        ...,\n",
      "        [15764.8096],\n",
      "        [ 6829.6455],\n",
      "        [11336.1279]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[56/1000]\n",
      " - TRAIN LOSS : 289.2429220585033 R2 : 0.9986463237772398\n",
      "- VAL LOSS : 257.9560241699219 R2 : 0.9996294379234314\n",
      "성능 및 손실 개선이 없어서 55 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10532.6465],\n",
      "        [14356.6396],\n",
      "        [10578.4961],\n",
      "        ...,\n",
      "        [15739.2646],\n",
      "        [ 6851.7754],\n",
      "        [11322.5322]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[57/1000]\n",
      " - TRAIN LOSS : 289.66674425107715 R2 : 0.9987260505770766\n",
      "- VAL LOSS : 254.5936737060547 R2 : 0.9996590614318848\n",
      "성능 및 손실 개선이 없어서 56 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10506.9873],\n",
      "        [14276.6406],\n",
      "        [10572.2266],\n",
      "        ...,\n",
      "        [15972.9512],\n",
      "        [ 6856.0928],\n",
      "        [11319.1943]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[58/1000]\n",
      " - TRAIN LOSS : 285.7926731154381 R2 : 0.9987533855158471\n",
      "- VAL LOSS : 243.30984497070312 R2 : 0.9996804594993591\n",
      "성능 및 손실 개선이 없어서 57 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10510.3740],\n",
      "        [14344.3389],\n",
      "        [10566.7988],\n",
      "        ...,\n",
      "        [15752.2217],\n",
      "        [ 6816.5469],\n",
      "        [11352.5439]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[59/1000]\n",
      " - TRAIN LOSS : 286.1646784000322 R2 : 0.9987478402478444\n",
      "- VAL LOSS : 245.31602478027344 R2 : 0.9996760487556458\n",
      "성능 및 손실 개선이 없어서 58 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10505.7412],\n",
      "        [14343.6436],\n",
      "        [10544.3936],\n",
      "        ...,\n",
      "        [15907.0234],\n",
      "        [ 6830.0400],\n",
      "        [11402.0605]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[60/1000]\n",
      " - TRAIN LOSS : 283.5279923151804 R2 : 0.9987581769366308\n",
      "- VAL LOSS : 259.1275634765625 R2 : 0.9996442794799805\n",
      "성능 및 손실 개선이 없어서 59 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10566.9990],\n",
      "        [14323.9590],\n",
      "        [10524.0449],\n",
      "        ...,\n",
      "        [15918.8672],\n",
      "        [ 6836.4512],\n",
      "        [11239.5312]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[61/1000]\n",
      " - TRAIN LOSS : 286.8205006426596 R2 : 0.9987237953145004\n",
      "- VAL LOSS : 249.49301147460938 R2 : 0.9996539354324341\n",
      "성능 및 손실 개선이 없어서 60 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10502.6914],\n",
      "        [14249.5674],\n",
      "        [10568.6367],\n",
      "        ...,\n",
      "        [15855.6025],\n",
      "        [ 6889.6978],\n",
      "        [11357.2197]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[62/1000]\n",
      " - TRAIN LOSS : 288.0044300929819 R2 : 0.9987262148745212\n",
      "- VAL LOSS : 240.92926025390625 R2 : 0.9996362924575806\n",
      "성능 및 손실 개선이 없어서 61 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10539.6113],\n",
      "        [14113.8154],\n",
      "        [10548.2441],\n",
      "        ...,\n",
      "        [15844.8740],\n",
      "        [ 6861.2202],\n",
      "        [11319.7100]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[63/1000]\n",
      " - TRAIN LOSS : 283.90438893901324 R2 : 0.9987526086660532\n",
      "- VAL LOSS : 250.4162139892578 R2 : 0.9996610879898071\n",
      "성능 및 손실 개선이 없어서 62 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10491.7168],\n",
      "        [14179.6895],\n",
      "        [10581.9717],\n",
      "        ...,\n",
      "        [15881.4775],\n",
      "        [ 6923.9639],\n",
      "        [11325.5771]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[64/1000]\n",
      " - TRAIN LOSS : 282.80802739428105 R2 : 0.9988044510453434\n",
      "- VAL LOSS : 241.5823516845703 R2 : 0.999720573425293\n",
      "성능 및 손실 개선이 없어서 63 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10601.2900],\n",
      "        [14137.3652],\n",
      "        [10579.1807],\n",
      "        ...,\n",
      "        [15840.8555],\n",
      "        [ 6787.7583],\n",
      "        [11334.9736]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[65/1000]\n",
      " - TRAIN LOSS : 282.5649443704737 R2 : 0.9988074826137339\n",
      "- VAL LOSS : 244.2144317626953 R2 : 0.999666154384613\n",
      "성능 및 손실 개선이 없어서 64 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10514.2002],\n",
      "        [14298.8223],\n",
      "        [10543.7012],\n",
      "        ...,\n",
      "        [15772.9404],\n",
      "        [ 6825.1099],\n",
      "        [11297.5781]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[66/1000]\n",
      " - TRAIN LOSS : 286.77942558795877 R2 : 0.9987353006117968\n",
      "- VAL LOSS : 241.24398803710938 R2 : 0.9996827244758606\n",
      "성능 및 손실 개선이 없어서 65 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10539.8545],\n",
      "        [14351.8408],\n",
      "        [10529.1846],\n",
      "        ...,\n",
      "        [15861.4844],\n",
      "        [ 6831.4805],\n",
      "        [11310.8799]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[67/1000]\n",
      " - TRAIN LOSS : 284.96212623924447 R2 : 0.9987711441874348\n",
      "- VAL LOSS : 254.55648803710938 R2 : 0.99965500831604\n",
      "성능 및 손실 개선이 없어서 66 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10608.9209],\n",
      "        [14274.0596],\n",
      "        [10509.1289],\n",
      "        ...,\n",
      "        [15855.2002],\n",
      "        [ 6908.0288],\n",
      "        [11315.8477]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[68/1000]\n",
      " - TRAIN LOSS : 282.9132925259865 R2 : 0.998785111117705\n",
      "- VAL LOSS : 242.7312774658203 R2 : 0.9996940493583679\n",
      "성능 및 손실 개선이 없어서 67 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10544.1816],\n",
      "        [14222.9512],\n",
      "        [10529.9473],\n",
      "        ...,\n",
      "        [15929.3174],\n",
      "        [ 6912.7061],\n",
      "        [11379.4082]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[69/1000]\n",
      " - TRAIN LOSS : 281.8352737615752 R2 : 0.9987831452057601\n",
      "- VAL LOSS : 241.92742919921875 R2 : 0.9996938109397888\n",
      "성능 및 손실 개선이 없어서 68 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10524.5137],\n",
      "        [14097.4326],\n",
      "        [10533.7725],\n",
      "        ...,\n",
      "        [15823.1240],\n",
      "        [ 6869.1787],\n",
      "        [11267.0303]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[70/1000]\n",
      " - TRAIN LOSS : 284.29376695078383 R2 : 0.9988012456365484\n",
      "- VAL LOSS : 245.63705444335938 R2 : 0.9996886849403381\n",
      "성능 및 손실 개선이 없어서 69 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10523.7725],\n",
      "        [14263.0244],\n",
      "        [10545.8916],\n",
      "        ...,\n",
      "        [15873.5762],\n",
      "        [ 6913.1953],\n",
      "        [11355.0654]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[71/1000]\n",
      " - TRAIN LOSS : 282.3105729830467 R2 : 0.9988440691414526\n",
      "- VAL LOSS : 248.5287322998047 R2 : 0.9996768236160278\n",
      "성능 및 손실 개선이 없어서 70 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10565.2939],\n",
      "        [14224.1973],\n",
      "        [10551.1934],\n",
      "        ...,\n",
      "        [15794.5146],\n",
      "        [ 6949.5620],\n",
      "        [11366.5742]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[72/1000]\n",
      " - TRAIN LOSS : 282.6442814614347 R2 : 0.9987798059468475\n",
      "- VAL LOSS : 246.68569946289062 R2 : 0.999677836894989\n",
      "성능 및 손실 개선이 없어서 71 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10511.1650],\n",
      "        [14158.7871],\n",
      "        [10545.0430],\n",
      "        ...,\n",
      "        [15937.2129],\n",
      "        [ 6899.6582],\n",
      "        [11311.1973]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[73/1000]\n",
      " - TRAIN LOSS : 286.18586391165485 R2 : 0.9987950706575187\n",
      "- VAL LOSS : 264.9229431152344 R2 : 0.9996141791343689\n",
      "성능 및 손실 개선이 없어서 72 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10574.7119],\n",
      "        [14202.3359],\n",
      "        [10626.0498],\n",
      "        ...,\n",
      "        [15805.4980],\n",
      "        [ 6955.6626],\n",
      "        [11351.4697]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[74/1000]\n",
      " - TRAIN LOSS : 283.6123317454909 R2 : 0.998762161697527\n",
      "- VAL LOSS : 243.94837951660156 R2 : 0.9996929168701172\n",
      "성능 및 손실 개선이 없어서 73 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10485.6123],\n",
      "        [14190.9844],\n",
      "        [10513.2822],\n",
      "        ...,\n",
      "        [15789.8945],\n",
      "        [ 6925.3677],\n",
      "        [11318.2627]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[75/1000]\n",
      " - TRAIN LOSS : 282.85745333158053 R2 : 0.9987666379053247\n",
      "- VAL LOSS : 235.81956481933594 R2 : 0.9997082352638245\n",
      "성능 및 손실 개선이 없어서 74 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10559.5439],\n",
      "        [14199.2285],\n",
      "        [10587.5234],\n",
      "        ...,\n",
      "        [15717.0889],\n",
      "        [ 6876.7837],\n",
      "        [11406.4590]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[76/1000]\n",
      " - TRAIN LOSS : 283.1708120254059 R2 : 0.9988324374698754\n",
      "- VAL LOSS : 261.63677978515625 R2 : 0.9996166229248047\n",
      "성능 및 손실 개선이 없어서 75 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10548.2275],\n",
      "        [14277.8408],\n",
      "        [10558.9326],\n",
      "        ...,\n",
      "        [15865.2344],\n",
      "        [ 7010.3013],\n",
      "        [11373.2793]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[77/1000]\n",
      " - TRAIN LOSS : 280.5407389933003 R2 : 0.9988450244477084\n",
      "- VAL LOSS : 244.1868896484375 R2 : 0.999700665473938\n",
      "성능 및 손실 개선이 없어서 76 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10589.8340],\n",
      "        [14387.9414],\n",
      "        [10588.4473],\n",
      "        ...,\n",
      "        [15901.5479],\n",
      "        [ 6885.9819],\n",
      "        [11347.4639]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[78/1000]\n",
      " - TRAIN LOSS : 278.93802348389306 R2 : 0.9988306230640784\n",
      "- VAL LOSS : 244.9685821533203 R2 : 0.999689519405365\n",
      "성능 및 손실 개선이 없어서 77 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10568.5410],\n",
      "        [14306.6016],\n",
      "        [10572.7402],\n",
      "        ...,\n",
      "        [15901.4189],\n",
      "        [ 6965.4277],\n",
      "        [11380.4434]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[79/1000]\n",
      " - TRAIN LOSS : 280.7153155052056 R2 : 0.9988324405783314\n",
      "- VAL LOSS : 248.80194091796875 R2 : 0.9996787905693054\n",
      "성능 및 손실 개선이 없어서 78 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10602.7012],\n",
      "        [14255.7363],\n",
      "        [10541.7666],\n",
      "        ...,\n",
      "        [15938.2031],\n",
      "        [ 6913.5542],\n",
      "        [11372.5264]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[80/1000]\n",
      " - TRAIN LOSS : 282.0178617912601 R2 : 0.9988172176608517\n",
      "- VAL LOSS : 236.56056213378906 R2 : 0.999707043170929\n",
      "성능 및 손실 개선이 없어서 79 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10575.0654],\n",
      "        [14206.3584],\n",
      "        [10605.9346],\n",
      "        ...,\n",
      "        [15855.3457],\n",
      "        [ 6915.5288],\n",
      "        [11321.8604]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[81/1000]\n",
      " - TRAIN LOSS : 277.92278681908005 R2 : 0.9988712118999278\n",
      "- VAL LOSS : 239.25863647460938 R2 : 0.9997033476829529\n",
      "성능 및 손실 개선이 없어서 80 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10573.0488],\n",
      "        [14153.8213],\n",
      "        [10504.9521],\n",
      "        ...,\n",
      "        [15794.7930],\n",
      "        [ 6823.4844],\n",
      "        [11267.9307]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[82/1000]\n",
      " - TRAIN LOSS : 277.49546926366617 R2 : 0.9988682663580585\n",
      "- VAL LOSS : 251.76025390625 R2 : 0.99962317943573\n",
      "성능 및 손실 개선이 없어서 81 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10534.0088],\n",
      "        [14135.5879],\n",
      "        [10580.7275],\n",
      "        ...,\n",
      "        [15908.5098],\n",
      "        [ 6863.3447],\n",
      "        [11342.6641]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[83/1000]\n",
      " - TRAIN LOSS : 278.3647068366855 R2 : 0.9988167999309853\n",
      "- VAL LOSS : 245.81129455566406 R2 : 0.9996841549873352\n",
      "성능 및 손실 개선이 없어서 82 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10535.5977],\n",
      "        [14331.2461],\n",
      "        [10500.4219],\n",
      "        ...,\n",
      "        [15889.4619],\n",
      "        [ 6829.4219],\n",
      "        [11301.5234]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[84/1000]\n",
      " - TRAIN LOSS : 277.5330609785376 R2 : 0.9988874854755526\n",
      "- VAL LOSS : 235.95492553710938 R2 : 0.9996928572654724\n",
      "성능 및 손실 개선이 없어서 83 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10514.0254],\n",
      "        [14231.6152],\n",
      "        [10559.4766],\n",
      "        ...,\n",
      "        [15848.4297],\n",
      "        [ 6855.0361],\n",
      "        [11285.9033]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[85/1000]\n",
      " - TRAIN LOSS : 283.0190278094315 R2 : 0.9987818553985486\n",
      "- VAL LOSS : 236.60415649414062 R2 : 0.9996891021728516\n",
      "성능 및 손실 개선이 없어서 84 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10536.0137],\n",
      "        [14030.0879],\n",
      "        [10565.6553],\n",
      "        ...,\n",
      "        [15847.0811],\n",
      "        [ 6910.5845],\n",
      "        [11347.2637]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[86/1000]\n",
      " - TRAIN LOSS : 277.33974998503965 R2 : 0.9988536121015288\n",
      "- VAL LOSS : 242.07342529296875 R2 : 0.9997076392173767\n",
      "성능 및 손실 개선이 없어서 85 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10504.7070],\n",
      "        [14268.9082],\n",
      "        [10508.8203],\n",
      "        ...,\n",
      "        [15725.3330],\n",
      "        [ 6806.4678],\n",
      "        [11276.4648]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[87/1000]\n",
      " - TRAIN LOSS : 277.6874647098228 R2 : 0.9988590561737448\n",
      "- VAL LOSS : 243.5000762939453 R2 : 0.9996935129165649\n",
      "성능 및 손실 개선이 없어서 86 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10601.1914],\n",
      "        [14323.8994],\n",
      "        [10522.7490],\n",
      "        ...,\n",
      "        [15824.3848],\n",
      "        [ 6819.8354],\n",
      "        [11227.6768]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[88/1000]\n",
      " - TRAIN LOSS : 274.7019856558629 R2 : 0.9989051975669264\n",
      "- VAL LOSS : 246.99830627441406 R2 : 0.9996941089630127\n",
      "성능 및 손실 개선이 없어서 87 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10505.3867],\n",
      "        [14377.7002],\n",
      "        [10603.4355],\n",
      "        ...,\n",
      "        [15885.5654],\n",
      "        [ 6859.7598],\n",
      "        [11341.6641]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[89/1000]\n",
      " - TRAIN LOSS : 275.90205334659663 R2 : 0.9989008394338317\n",
      "- VAL LOSS : 240.6699981689453 R2 : 0.9997036457061768\n",
      "성능 및 손실 개선이 없어서 88 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10487.6445],\n",
      "        [14151.4795],\n",
      "        [10521.6562],\n",
      "        ...,\n",
      "        [15842.9912],\n",
      "        [ 6952.0400],\n",
      "        [11302.0938]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[90/1000]\n",
      " - TRAIN LOSS : 274.067389878891 R2 : 0.9989425109449235\n",
      "- VAL LOSS : 241.3217315673828 R2 : 0.9996710419654846\n",
      "성능 및 손실 개선이 없어서 89 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10523.5859],\n",
      "        [14262.2695],\n",
      "        [10570.8184],\n",
      "        ...,\n",
      "        [15823.1660],\n",
      "        [ 6901.4355],\n",
      "        [11395.8242]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[91/1000]\n",
      " - TRAIN LOSS : 277.1774474839033 R2 : 0.99889309566366\n",
      "- VAL LOSS : 267.9391784667969 R2 : 0.9996389746665955\n",
      "성능 및 손실 개선이 없어서 90 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10510.7773],\n",
      "        [14154.4082],\n",
      "        [10504.5088],\n",
      "        ...,\n",
      "        [15808.3516],\n",
      "        [ 6941.8037],\n",
      "        [11326.7070]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[92/1000]\n",
      " - TRAIN LOSS : 276.0976075273138 R2 : 0.9988991346321952\n",
      "- VAL LOSS : 239.5440216064453 R2 : 0.9996746778488159\n",
      "성능 및 손실 개선이 없어서 91 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10535.1123],\n",
      "        [14239.9795],\n",
      "        [10532.5283],\n",
      "        ...,\n",
      "        [15798.7422],\n",
      "        [ 6872.2021],\n",
      "        [11349.5469]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[93/1000]\n",
      " - TRAIN LOSS : 277.09272627451276 R2 : 0.9989032493110119\n",
      "- VAL LOSS : 235.55470275878906 R2 : 0.9997022151947021\n",
      "성능 및 손실 개선이 없어서 92 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10520.8018],\n",
      "        [14181.4658],\n",
      "        [10561.6436],\n",
      "        ...,\n",
      "        [15776.2715],\n",
      "        [ 6855.5957],\n",
      "        [11249.5146]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[94/1000]\n",
      " - TRAIN LOSS : 277.8079532886888 R2 : 0.9989176563364897\n",
      "- VAL LOSS : 262.9965515136719 R2 : 0.9996401071548462\n",
      "성능 및 손실 개선이 없어서 93 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10491.2832],\n",
      "        [14384.2559],\n",
      "        [10549.6348],\n",
      "        ...,\n",
      "        [15630.2246],\n",
      "        [ 7071.8750],\n",
      "        [11374.4463]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[95/1000]\n",
      " - TRAIN LOSS : 274.4977797049432 R2 : 0.9988986231979211\n",
      "- VAL LOSS : 242.71788024902344 R2 : 0.9996911883354187\n",
      "성능 및 손실 개선이 없어서 94 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10578.7627],\n",
      "        [14218.0508],\n",
      "        [10547.6113],\n",
      "        ...,\n",
      "        [15825.9717],\n",
      "        [ 6874.0488],\n",
      "        [11318.8154]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[96/1000]\n",
      " - TRAIN LOSS : 274.90032545584586 R2 : 0.9989606693328748\n",
      "- VAL LOSS : 237.4620819091797 R2 : 0.9997061491012573\n",
      "성능 및 손실 개선이 없어서 95 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10606.3740],\n",
      "        [14275.1357],\n",
      "        [10518.5088],\n",
      "        ...,\n",
      "        [15861.1504],\n",
      "        [ 6742.5723],\n",
      "        [11307.2119]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[97/1000]\n",
      " - TRAIN LOSS : 277.6288763570972 R2 : 0.9989234169272432\n",
      "- VAL LOSS : 235.08566284179688 R2 : 0.9996897578239441\n",
      "성능 및 손실 개선이 없어서 96 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10567.5117],\n",
      "        [14699.6387],\n",
      "        [10547.6572],\n",
      "        ...,\n",
      "        [15813.0635],\n",
      "        [ 6838.7349],\n",
      "        [11304.7383]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[98/1000]\n",
      " - TRAIN LOSS : 276.67895753703806 R2 : 0.9989173345646616\n",
      "- VAL LOSS : 239.4527587890625 R2 : 0.999650776386261\n",
      "성능 및 손실 개선이 없어서 97 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10499.8027],\n",
      "        [14152.7441],\n",
      "        [10582.1719],\n",
      "        ...,\n",
      "        [15802.3174],\n",
      "        [ 6906.0742],\n",
      "        [11362.6963]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[99/1000]\n",
      " - TRAIN LOSS : 276.6957049234283 R2 : 0.998898046874627\n",
      "- VAL LOSS : 254.5309295654297 R2 : 0.9996016025543213\n",
      "성능 및 손실 개선이 없어서 98 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10555.3096],\n",
      "        [14140.3975],\n",
      "        [10532.9209],\n",
      "        ...,\n",
      "        [15694.7998],\n",
      "        [ 6853.8662],\n",
      "        [11263.1250]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[100/1000]\n",
      " - TRAIN LOSS : 275.27679628275206 R2 : 0.9989147510957531\n",
      "- VAL LOSS : 240.2128448486328 R2 : 0.9997161626815796\n",
      "성능 및 손실 개선이 없어서 99 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10460.3721],\n",
      "        [14239.9287],\n",
      "        [10553.2529],\n",
      "        ...,\n",
      "        [15795.1172],\n",
      "        [ 6869.2344],\n",
      "        [11312.0254]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[101/1000]\n",
      " - TRAIN LOSS : 275.8255359455691 R2 : 0.9989042977310688\n",
      "- VAL LOSS : 244.67369079589844 R2 : 0.9996485710144043\n",
      "성능 및 손실 개선이 없어서 100 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10531.5400],\n",
      "        [14160.7021],\n",
      "        [10561.8643],\n",
      "        ...,\n",
      "        [15734.8789],\n",
      "        [ 6889.4888],\n",
      "        [11284.9336]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[102/1000]\n",
      " - TRAIN LOSS : 274.7639017415948 R2 : 0.9989626099109028\n",
      "- VAL LOSS : 240.64419555664062 R2 : 0.9996647834777832\n",
      "성능 및 손실 개선이 없어서 101 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10555.2910],\n",
      "        [14202.4434],\n",
      "        [10538.3486],\n",
      "        ...,\n",
      "        [15739.3740],\n",
      "        [ 6843.9814],\n",
      "        [11341.6963]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[103/1000]\n",
      " - TRAIN LOSS : 220.8810603324562 R2 : 0.9991416704390474\n",
      "- VAL LOSS : 191.52040100097656 R2 : 0.9997825026512146\n",
      "성능 및 손실 개선이 없어서 102 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10542.1963],\n",
      "        [14200.8184],\n",
      "        [10531.7646],\n",
      "        ...,\n",
      "        [15726.8818],\n",
      "        [ 6850.3340],\n",
      "        [11342.8975]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[104/1000]\n",
      " - TRAIN LOSS : 204.18784521766872 R2 : 0.9992438620973908\n",
      "- VAL LOSS : 185.61148071289062 R2 : 0.9998083710670471\n",
      "성능 및 손실 개선이 없어서 103 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10539.0107],\n",
      "        [14210.0527],\n",
      "        [10525.2168],\n",
      "        ...,\n",
      "        [15714.0596],\n",
      "        [ 6830.2622],\n",
      "        [11331.4766]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[105/1000]\n",
      " - TRAIN LOSS : 197.08548168495705 R2 : 0.99927640910876\n",
      "- VAL LOSS : 182.66648864746094 R2 : 0.999812662601471\n",
      "성능 및 손실 개선이 없어서 104 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10535.1699],\n",
      "        [14210.6309],\n",
      "        [10525.3838],\n",
      "        ...,\n",
      "        [15737.7334],\n",
      "        [ 6829.6978],\n",
      "        [11329.1953]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[106/1000]\n",
      " - TRAIN LOSS : 193.3068854201415 R2 : 0.999287745399301\n",
      "- VAL LOSS : 183.11766052246094 R2 : 0.9998098611831665\n",
      "성능 및 손실 개선이 없어서 105 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10527.4180],\n",
      "        [14207.7393],\n",
      "        [10528.3574],\n",
      "        ...,\n",
      "        [15729.3018],\n",
      "        [ 6840.5649],\n",
      "        [11327.8301]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[107/1000]\n",
      " - TRAIN LOSS : 190.24261372055193 R2 : 0.9992941306032134\n",
      "- VAL LOSS : 181.54693603515625 R2 : 0.9998130798339844\n",
      "성능 및 손실 개선이 없어서 106 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10537.8730],\n",
      "        [14214.0840],\n",
      "        [10535.6631],\n",
      "        ...,\n",
      "        [15737.5225],\n",
      "        [ 6858.8936],\n",
      "        [11332.2461]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[108/1000]\n",
      " - TRAIN LOSS : 188.1211836654484 R2 : 0.9993004199423411\n",
      "- VAL LOSS : 180.92916870117188 R2 : 0.9998184442520142\n",
      "성능 및 손실 개선이 없어서 107 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10537.1895],\n",
      "        [14222.6123],\n",
      "        [10527.9189],\n",
      "        ...,\n",
      "        [15726.3662],\n",
      "        [ 6851.3574],\n",
      "        [11328.7627]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[109/1000]\n",
      " - TRAIN LOSS : 185.97498539174862 R2 : 0.9993008903760798\n",
      "- VAL LOSS : 178.72727966308594 R2 : 0.9998190402984619\n",
      "성능 및 손실 개선이 없어서 108 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10535.5127],\n",
      "        [14219.1143],\n",
      "        [10532.8330],\n",
      "        ...,\n",
      "        [15726.4834],\n",
      "        [ 6859.9414],\n",
      "        [11335.6182]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[110/1000]\n",
      " - TRAIN LOSS : 183.75298725212724 R2 : 0.9993120068055243\n",
      "- VAL LOSS : 178.606689453125 R2 : 0.9998212456703186\n",
      "성능 및 손실 개선이 없어서 109 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10537.7803],\n",
      "        [14216.0342],\n",
      "        [10531.3115],\n",
      "        ...,\n",
      "        [15715.4248],\n",
      "        [ 6862.5640],\n",
      "        [11326.0635]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[111/1000]\n",
      " - TRAIN LOSS : 182.1150691075928 R2 : 0.9993171692205377\n",
      "- VAL LOSS : 178.7715606689453 R2 : 0.9998112320899963\n",
      "성능 및 손실 개선이 없어서 110 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10534.7812],\n",
      "        [14217.8105],\n",
      "        [10519.4707],\n",
      "        ...,\n",
      "        [15721.7754],\n",
      "        [ 6847.1973],\n",
      "        [11327.8848]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[112/1000]\n",
      " - TRAIN LOSS : 180.15005924568027 R2 : 0.9993216280365239\n",
      "- VAL LOSS : 176.6949005126953 R2 : 0.9998289942741394\n",
      "성능 및 손실 개선이 없어서 111 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10531.6719],\n",
      "        [14221.5654],\n",
      "        [10527.8330],\n",
      "        ...,\n",
      "        [15730.8682],\n",
      "        [ 6868.5586],\n",
      "        [11331.2676]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[113/1000]\n",
      " - TRAIN LOSS : 178.443709699489 R2 : 0.9993309571037392\n",
      "- VAL LOSS : 175.32000732421875 R2 : 0.9998331069946289\n",
      "성능 및 손실 개선이 없어서 112 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10532.1943],\n",
      "        [14224.2051],\n",
      "        [10517.2891],\n",
      "        ...,\n",
      "        [15729.5742],\n",
      "        [ 6845.6182],\n",
      "        [11320.2568]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[114/1000]\n",
      " - TRAIN LOSS : 177.18929920917694 R2 : 0.9993342454150572\n",
      "- VAL LOSS : 176.63096618652344 R2 : 0.9998281002044678\n",
      "성능 및 손실 개선이 없어서 113 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10533.9805],\n",
      "        [14224.5537],\n",
      "        [10524.2031],\n",
      "        ...,\n",
      "        [15730.0312],\n",
      "        [ 6857.4673],\n",
      "        [11335.6377]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[115/1000]\n",
      " - TRAIN LOSS : 175.95108533160473 R2 : 0.9993360855122433\n",
      "- VAL LOSS : 175.09483337402344 R2 : 0.9998323917388916\n",
      "성능 및 손실 개선이 없어서 114 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10537.1514],\n",
      "        [14223.3242],\n",
      "        [10528.8857],\n",
      "        ...,\n",
      "        [15725.1416],\n",
      "        [ 6850.3228],\n",
      "        [11327.0625]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[116/1000]\n",
      " - TRAIN LOSS : 175.55063064853712 R2 : 0.9993359656346357\n",
      "- VAL LOSS : 174.75070190429688 R2 : 0.9998348951339722\n",
      "성능 및 손실 개선이 없어서 115 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10539.9678],\n",
      "        [14223.4492],\n",
      "        [10530.2246],\n",
      "        ...,\n",
      "        [15728.8525],\n",
      "        [ 6861.2485],\n",
      "        [11338.0449]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[117/1000]\n",
      " - TRAIN LOSS : 174.86286381170677 R2 : 0.9993390739352638\n",
      "- VAL LOSS : 175.22076416015625 R2 : 0.999835729598999\n",
      "성능 및 손실 개선이 없어서 116 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10542.3545],\n",
      "        [14223.5391],\n",
      "        [10519.1514],\n",
      "        ...,\n",
      "        [15717.0850],\n",
      "        [ 6858.7236],\n",
      "        [11325.2520]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[118/1000]\n",
      " - TRAIN LOSS : 173.68084848993294 R2 : 0.9993408991274032\n",
      "- VAL LOSS : 174.6667938232422 R2 : 0.9998356103897095\n",
      "성능 및 손실 개선이 없어서 117 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10542.2256],\n",
      "        [14228.9297],\n",
      "        [10524.6631],\n",
      "        ...,\n",
      "        [15719.1895],\n",
      "        [ 6867.7671],\n",
      "        [11329.0293]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[119/1000]\n",
      " - TRAIN LOSS : 172.9322175813001 R2 : 0.9993408711046718\n",
      "- VAL LOSS : 175.0475311279297 R2 : 0.9998329877853394\n",
      "성능 및 손실 개선이 없어서 118 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10536.6396],\n",
      "        [14218.9346],\n",
      "        [10517.9199],\n",
      "        ...,\n",
      "        [15717.5303],\n",
      "        [ 6857.0371],\n",
      "        [11336.9043]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[120/1000]\n",
      " - TRAIN LOSS : 171.54755731157405 R2 : 0.9993473039767578\n",
      "- VAL LOSS : 172.8407440185547 R2 : 0.9998407363891602\n",
      "성능 및 손실 개선이 없어서 119 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10541.9033],\n",
      "        [14220.2969],\n",
      "        [10516.2754],\n",
      "        ...,\n",
      "        [15718.1406],\n",
      "        [ 6845.5869],\n",
      "        [11329.5625]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[121/1000]\n",
      " - TRAIN LOSS : 171.1467292507127 R2 : 0.9993518833852965\n",
      "- VAL LOSS : 173.53372192382812 R2 : 0.9998394846916199\n",
      "성능 및 손실 개선이 없어서 120 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10534.1973],\n",
      "        [14210.0791],\n",
      "        [10520.9951],\n",
      "        ...,\n",
      "        [15726.9375],\n",
      "        [ 6853.0205],\n",
      "        [11328.3604]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[122/1000]\n",
      " - TRAIN LOSS : 170.2561554949784 R2 : 0.9993533336976361\n",
      "- VAL LOSS : 173.31680297851562 R2 : 0.9998421669006348\n",
      "성능 및 손실 개선이 없어서 121 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10544.5039],\n",
      "        [14207.0918],\n",
      "        [10508.1895],\n",
      "        ...,\n",
      "        [15713.4043],\n",
      "        [ 6851.3462],\n",
      "        [11331.3555]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[123/1000]\n",
      " - TRAIN LOSS : 169.42334687731568 R2 : 0.9993563947031053\n",
      "- VAL LOSS : 172.4031524658203 R2 : 0.99984210729599\n",
      "성능 및 손실 개선이 없어서 122 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10548.3721],\n",
      "        [14211.1387],\n",
      "        [10518.2617],\n",
      "        ...,\n",
      "        [15726.4541],\n",
      "        [ 6869.4570],\n",
      "        [11335.7734]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[124/1000]\n",
      " - TRAIN LOSS : 168.82999123047352 R2 : 0.999358749203141\n",
      "- VAL LOSS : 172.45089721679688 R2 : 0.9998417496681213\n",
      "성능 및 손실 개선이 없어서 123 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10538.9189],\n",
      "        [14199.9834],\n",
      "        [10511.1641],\n",
      "        ...,\n",
      "        [15718.0537],\n",
      "        [ 6860.2563],\n",
      "        [11330.3877]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[125/1000]\n",
      " - TRAIN LOSS : 168.6907332001329 R2 : 0.9993584186810092\n",
      "- VAL LOSS : 172.2482452392578 R2 : 0.9998422861099243\n",
      "성능 및 손실 개선이 없어서 124 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10544.0820],\n",
      "        [14205.2383],\n",
      "        [10512.1895],\n",
      "        ...,\n",
      "        [15724.9551],\n",
      "        [ 6842.0649],\n",
      "        [11332.5117]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[126/1000]\n",
      " - TRAIN LOSS : 168.23127589120082 R2 : 0.9993576730711985\n",
      "- VAL LOSS : 173.1004180908203 R2 : 0.9998382329940796\n",
      "성능 및 손실 개선이 없어서 125 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10541.5967],\n",
      "        [14216.0146],\n",
      "        [10520.1621],\n",
      "        ...,\n",
      "        [15739.3779],\n",
      "        [ 6861.0137],\n",
      "        [11338.2842]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[127/1000]\n",
      " - TRAIN LOSS : 167.13586325813304 R2 : 0.9993651019703145\n",
      "- VAL LOSS : 171.6803436279297 R2 : 0.9998432397842407\n",
      "성능 및 손실 개선이 없어서 126 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10544.1855],\n",
      "        [14213.9307],\n",
      "        [10518.3789],\n",
      "        ...,\n",
      "        [15726.8525],\n",
      "        [ 6861.0942],\n",
      "        [11333.4014]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[128/1000]\n",
      " - TRAIN LOSS : 167.27839272112394 R2 : 0.9993593638847829\n",
      "- VAL LOSS : 172.00428771972656 R2 : 0.9998431205749512\n",
      "성능 및 손실 개선이 없어서 127 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10541.5293],\n",
      "        [14211.4502],\n",
      "        [10520.3721],\n",
      "        ...,\n",
      "        [15729.7266],\n",
      "        [ 6853.4966],\n",
      "        [11336.6934]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[129/1000]\n",
      " - TRAIN LOSS : 167.27601911999878 R2 : 0.9993617922572766\n",
      "- VAL LOSS : 172.3461151123047 R2 : 0.9998398423194885\n",
      "성능 및 손실 개선이 없어서 128 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10538.3936],\n",
      "        [14201.1396],\n",
      "        [10527.1309],\n",
      "        ...,\n",
      "        [15724.8701],\n",
      "        [ 6863.0166],\n",
      "        [11337.3223]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[130/1000]\n",
      " - TRAIN LOSS : 166.05322824152384 R2 : 0.999361619691339\n",
      "- VAL LOSS : 174.36219787597656 R2 : 0.9998363256454468\n",
      "성능 및 손실 개선이 없어서 129 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10546.3477],\n",
      "        [14214.2637],\n",
      "        [10520.2383],\n",
      "        ...,\n",
      "        [15728.5996],\n",
      "        [ 6876.9604],\n",
      "        [11342.7529]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[131/1000]\n",
      " - TRAIN LOSS : 165.20519731563883 R2 : 0.9993678959283244\n",
      "- VAL LOSS : 172.7339630126953 R2 : 0.9998409152030945\n",
      "성능 및 손실 개선이 없어서 130 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10543.4648],\n",
      "        [14211.8770],\n",
      "        [10509.1328],\n",
      "        ...,\n",
      "        [15728.0947],\n",
      "        [ 6853.2734],\n",
      "        [11340.7471]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[132/1000]\n",
      " - TRAIN LOSS : 165.07361549069083 R2 : 0.9993684809086687\n",
      "- VAL LOSS : 172.55111694335938 R2 : 0.9998425245285034\n",
      "성능 및 손실 개선이 없어서 131 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10543.6357],\n",
      "        [14199.2578],\n",
      "        [10512.1094],\n",
      "        ...,\n",
      "        [15727.4795],\n",
      "        [ 6860.2178],\n",
      "        [11331.4668]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[133/1000]\n",
      " - TRAIN LOSS : 164.4995467224519 R2 : 0.9993646268583494\n",
      "- VAL LOSS : 170.7765655517578 R2 : 0.9998428821563721\n",
      "성능 및 손실 개선이 없어서 132 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10544.1299],\n",
      "        [14209.6973],\n",
      "        [10515.0547],\n",
      "        ...,\n",
      "        [15728.4336],\n",
      "        [ 6856.9185],\n",
      "        [11342.2568]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[134/1000]\n",
      " - TRAIN LOSS : 163.82034221982397 R2 : 0.9993702896570755\n",
      "- VAL LOSS : 172.73175048828125 R2 : 0.999839723110199\n",
      "성능 및 손실 개선이 없어서 133 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10545.9482],\n",
      "        [14203.4033],\n",
      "        [10513.0322],\n",
      "        ...,\n",
      "        [15742.3066],\n",
      "        [ 6854.4897],\n",
      "        [11337.9648]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[135/1000]\n",
      " - TRAIN LOSS : 163.72903621529352 R2 : 0.9993664904710056\n",
      "- VAL LOSS : 172.3461456298828 R2 : 0.9998417496681213\n",
      "성능 및 손실 개선이 없어서 134 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10554.5557],\n",
      "        [14214.4512],\n",
      "        [10512.4258],\n",
      "        ...,\n",
      "        [15746.5986],\n",
      "        [ 6857.4438],\n",
      "        [11334.8096]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[136/1000]\n",
      " - TRAIN LOSS : 163.46003286667568 R2 : 0.9993712194118226\n",
      "- VAL LOSS : 171.0977020263672 R2 : 0.9998425245285034\n",
      "성능 및 손실 개선이 없어서 135 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10553.1846],\n",
      "        [14197.5039],\n",
      "        [10503.5840],\n",
      "        ...,\n",
      "        [15745.5938],\n",
      "        [ 6845.7749],\n",
      "        [11340.1875]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[137/1000]\n",
      " - TRAIN LOSS : 162.77426998929218 R2 : 0.9993733505226643\n",
      "- VAL LOSS : 171.03485107421875 R2 : 0.9998432397842407\n",
      "성능 및 손실 개선이 없어서 136 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10557.4033],\n",
      "        [14204.2852],\n",
      "        [10502.7520],\n",
      "        ...,\n",
      "        [15735.0420],\n",
      "        [ 6857.4727],\n",
      "        [11343.3535]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[138/1000]\n",
      " - TRAIN LOSS : 161.98472726913909 R2 : 0.9993758150931288\n",
      "- VAL LOSS : 169.54344177246094 R2 : 0.9998482465744019\n",
      "성능 및 손실 개선이 없어서 137 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10552.0957],\n",
      "        [14193.9131],\n",
      "        [10506.5850],\n",
      "        ...,\n",
      "        [15734.5186],\n",
      "        [ 6827.5552],\n",
      "        [11333.7725]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[139/1000]\n",
      " - TRAIN LOSS : 162.16692313004037 R2 : 0.9993745479310197\n",
      "- VAL LOSS : 171.4400177001953 R2 : 0.9998404383659363\n",
      "성능 및 손실 개선이 없어서 138 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10552.8652],\n",
      "        [14199.2510],\n",
      "        [10510.8760],\n",
      "        ...,\n",
      "        [15733.6016],\n",
      "        [ 6858.7295],\n",
      "        [11342.3438]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[140/1000]\n",
      " - TRAIN LOSS : 161.59640543812102 R2 : 0.9993777143753181\n",
      "- VAL LOSS : 169.94451904296875 R2 : 0.9998476505279541\n",
      "성능 및 손실 개선이 없어서 139 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10553.9375],\n",
      "        [14197.4756],\n",
      "        [10504.1592],\n",
      "        ...,\n",
      "        [15747.3457],\n",
      "        [ 6872.0107],\n",
      "        [11343.9521]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[141/1000]\n",
      " - TRAIN LOSS : 161.07319064979603 R2 : 0.999379865520159\n",
      "- VAL LOSS : 169.95880126953125 R2 : 0.9998451471328735\n",
      "성능 및 손실 개선이 없어서 140 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10553.1758],\n",
      "        [14206.1709],\n",
      "        [10505.3975],\n",
      "        ...,\n",
      "        [15741.7393],\n",
      "        [ 6856.4102],\n",
      "        [11345.0049]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[142/1000]\n",
      " - TRAIN LOSS : 161.00432262196844 R2 : 0.9993765921760569\n",
      "- VAL LOSS : 169.3421630859375 R2 : 0.9998462200164795\n",
      "성능 및 손실 개선이 없어서 141 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10550.5879],\n",
      "        [14199.5596],\n",
      "        [10509.0459],\n",
      "        ...,\n",
      "        [15745.7256],\n",
      "        [ 6859.4561],\n",
      "        [11341.2783]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[143/1000]\n",
      " - TRAIN LOSS : 160.50005514052887 R2 : 0.9993791998909225\n",
      "- VAL LOSS : 171.64768981933594 R2 : 0.9998432993888855\n",
      "성능 및 손실 개선이 없어서 142 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10547.3789],\n",
      "        [14198.5586],\n",
      "        [10518.5488],\n",
      "        ...,\n",
      "        [15752.0215],\n",
      "        [ 6862.0439],\n",
      "        [11342.6738]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[144/1000]\n",
      " - TRAIN LOSS : 159.78151812696396 R2 : 0.9993815930756939\n",
      "- VAL LOSS : 169.65806579589844 R2 : 0.9998472332954407\n",
      "성능 및 손실 개선이 없어서 143 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10547.2197],\n",
      "        [14194.5381],\n",
      "        [10510.4248],\n",
      "        ...,\n",
      "        [15745.9307],\n",
      "        [ 6867.6318],\n",
      "        [11343.2529]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[145/1000]\n",
      " - TRAIN LOSS : 159.32373680900562 R2 : 0.9993857507600001\n",
      "- VAL LOSS : 169.06964111328125 R2 : 0.9998475909233093\n",
      "성능 및 손실 개선이 없어서 144 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10546.3389],\n",
      "        [14201.7012],\n",
      "        [10512.9736],\n",
      "        ...,\n",
      "        [15751.1064],\n",
      "        [ 6840.3301],\n",
      "        [11342.7686]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[146/1000]\n",
      " - TRAIN LOSS : 158.92517566606304 R2 : 0.9993873135984488\n",
      "- VAL LOSS : 169.62075805664062 R2 : 0.9998490810394287\n",
      "성능 및 손실 개선이 없어서 145 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10552.9902],\n",
      "        [14202.1396],\n",
      "        [10516.3604],\n",
      "        ...,\n",
      "        [15743.2842],\n",
      "        [ 6849.4961],\n",
      "        [11346.5264]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[147/1000]\n",
      " - TRAIN LOSS : 159.1408575105232 R2 : 0.9993874690367937\n",
      "- VAL LOSS : 170.71743774414062 R2 : 0.9998463988304138\n",
      "성능 및 손실 개선이 없어서 146 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10549.1201],\n",
      "        [14190.1826],\n",
      "        [10514.3643],\n",
      "        ...,\n",
      "        [15752.9297],\n",
      "        [ 6870.2300],\n",
      "        [11339.9893]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[148/1000]\n",
      " - TRAIN LOSS : 158.7611655015212 R2 : 0.9993854555965558\n",
      "- VAL LOSS : 169.75575256347656 R2 : 0.9998481869697571\n",
      "성능 및 손실 개선이 없어서 147 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10559.9258],\n",
      "        [14183.7148],\n",
      "        [10518.4482],\n",
      "        ...,\n",
      "        [15744.7217],\n",
      "        [ 6868.7095],\n",
      "        [11339.5977]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[149/1000]\n",
      " - TRAIN LOSS : 158.3164926481682 R2 : 0.9993866059428865\n",
      "- VAL LOSS : 169.85862731933594 R2 : 0.9998492002487183\n",
      "성능 및 손실 개선이 없어서 148 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10551.8301],\n",
      "        [14207.3057],\n",
      "        [10511.6777],\n",
      "        ...,\n",
      "        [15748.2969],\n",
      "        [ 6833.2827],\n",
      "        [11329.1201]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[150/1000]\n",
      " - TRAIN LOSS : 158.3945962480647 R2 : 0.9993838377850618\n",
      "- VAL LOSS : 170.6407928466797 R2 : 0.9998434782028198\n",
      "성능 및 손실 개선이 없어서 149 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10555.2246],\n",
      "        [14218.9238],\n",
      "        [10514.8730],\n",
      "        ...,\n",
      "        [15749.3086],\n",
      "        [ 6855.3237],\n",
      "        [11339.2012]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[151/1000]\n",
      " - TRAIN LOSS : 158.02266222718796 R2 : 0.9993882083208961\n",
      "- VAL LOSS : 169.8892059326172 R2 : 0.9998475909233093\n",
      "성능 및 손실 개선이 없어서 150 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10553.5566],\n",
      "        [14201.6299],\n",
      "        [10511.6436],\n",
      "        ...,\n",
      "        [15751.3750],\n",
      "        [ 6848.7852],\n",
      "        [11339.0635]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[152/1000]\n",
      " - TRAIN LOSS : 157.57044399515124 R2 : 0.9993887779454675\n",
      "- VAL LOSS : 168.95465087890625 R2 : 0.9998493790626526\n",
      "성능 및 손실 개선이 없어서 151 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10561.8135],\n",
      "        [14198.7168],\n",
      "        [10513.0391],\n",
      "        ...,\n",
      "        [15751.1084],\n",
      "        [ 6856.7764],\n",
      "        [11333.7715]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[153/1000]\n",
      " - TRAIN LOSS : 157.0048020771927 R2 : 0.9993886434114896\n",
      "- VAL LOSS : 169.40298461914062 R2 : 0.9998502731323242\n",
      "성능 및 손실 개선이 없어서 152 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10557.3232],\n",
      "        [14203.1680],\n",
      "        [10502.1924],\n",
      "        ...,\n",
      "        [15754.9287],\n",
      "        [ 6837.6714],\n",
      "        [11335.4951]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[154/1000]\n",
      " - TRAIN LOSS : 156.9124862432169 R2 : 0.9993893724532296\n",
      "- VAL LOSS : 170.22616577148438 R2 : 0.9998469352722168\n",
      "성능 및 손실 개선이 없어서 153 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10556.2871],\n",
      "        [14200.7080],\n",
      "        [10506.9473],\n",
      "        ...,\n",
      "        [15750.9697],\n",
      "        [ 6819.8276],\n",
      "        [11339.5654]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[155/1000]\n",
      " - TRAIN LOSS : 156.55420451555912 R2 : 0.9993895373879076\n",
      "- VAL LOSS : 169.2305145263672 R2 : 0.999851405620575\n",
      "성능 및 손실 개선이 없어서 154 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10560.1621],\n",
      "        [14198.4609],\n",
      "        [10511.9307],\n",
      "        ...,\n",
      "        [15764.2402],\n",
      "        [ 6835.0679],\n",
      "        [11342.3457]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[156/1000]\n",
      " - TRAIN LOSS : 156.1337311497257 R2 : 0.9993926119773298\n",
      "- VAL LOSS : 171.10997009277344 R2 : 0.9998492002487183\n",
      "성능 및 손실 개선이 없어서 155 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10555.1104],\n",
      "        [14204.2490],\n",
      "        [10513.6543],\n",
      "        ...,\n",
      "        [15768.4102],\n",
      "        [ 6840.8696],\n",
      "        [11345.7627]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[157/1000]\n",
      " - TRAIN LOSS : 155.9674671128334 R2 : 0.9993920393997252\n",
      "- VAL LOSS : 171.78515625 R2 : 0.9998465776443481\n",
      "성능 및 손실 개선이 없어서 156 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10560.5664],\n",
      "        [14204.8193],\n",
      "        [10509.6406],\n",
      "        ...,\n",
      "        [15754.4854],\n",
      "        [ 6829.9722],\n",
      "        [11341.3975]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[158/1000]\n",
      " - TRAIN LOSS : 155.59136314242883 R2 : 0.999394406659973\n",
      "- VAL LOSS : 168.9208984375 R2 : 0.9998502135276794\n",
      "성능 및 손실 개선이 없어서 157 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10560.8379],\n",
      "        [14197.3223],\n",
      "        [10526.6045],\n",
      "        ...,\n",
      "        [15767.6465],\n",
      "        [ 6844.8789],\n",
      "        [11346.0508]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[159/1000]\n",
      " - TRAIN LOSS : 155.2011846758707 R2 : 0.9993962685008092\n",
      "- VAL LOSS : 171.1863555908203 R2 : 0.999846875667572\n",
      "성능 및 손실 개선이 없어서 158 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10565.9062],\n",
      "        [14197.3301],\n",
      "        [10519.2764],\n",
      "        ...,\n",
      "        [15763.6475],\n",
      "        [ 6852.6758],\n",
      "        [11338.8857]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[160/1000]\n",
      " - TRAIN LOSS : 155.03661985931993 R2 : 0.9993946704590958\n",
      "- VAL LOSS : 169.11390686035156 R2 : 0.9998491406440735\n",
      "성능 및 손실 개선이 없어서 159 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10559.5684],\n",
      "        [14202.1152],\n",
      "        [10508.4824],\n",
      "        ...,\n",
      "        [15753.6885],\n",
      "        [ 6839.2847],\n",
      "        [11337.7910]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[161/1000]\n",
      " - TRAIN LOSS : 154.83211587460752 R2 : 0.9993949683268969\n",
      "- VAL LOSS : 168.35247802734375 R2 : 0.9998509883880615\n",
      "성능 및 손실 개선이 없어서 160 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10560.8799],\n",
      "        [14190.9971],\n",
      "        [10514.7656],\n",
      "        ...,\n",
      "        [15763.4141],\n",
      "        [ 6849.0752],\n",
      "        [11340.0781]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[162/1000]\n",
      " - TRAIN LOSS : 154.75914725013942 R2 : 0.9993950073380203\n",
      "- VAL LOSS : 167.81564331054688 R2 : 0.9998521208763123\n",
      "성능 및 손실 개선이 없어서 161 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10562.8740],\n",
      "        [14191.4082],\n",
      "        [10516.0762],\n",
      "        ...,\n",
      "        [15777.8818],\n",
      "        [ 6839.1787],\n",
      "        [11341.1162]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[163/1000]\n",
      " - TRAIN LOSS : 154.74961519701384 R2 : 0.9993970115616859\n",
      "- VAL LOSS : 170.13345336914062 R2 : 0.9998477101325989\n",
      "성능 및 손실 개선이 없어서 162 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10571.6006],\n",
      "        [14201.8203],\n",
      "        [10507.1562],\n",
      "        ...,\n",
      "        [15757.3740],\n",
      "        [ 6846.6938],\n",
      "        [11337.1133]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[164/1000]\n",
      " - TRAIN LOSS : 154.3235361599083 R2 : 0.9993962464152291\n",
      "- VAL LOSS : 169.61460876464844 R2 : 0.9998474717140198\n",
      "성능 및 손실 개선이 없어서 163 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10568.3867],\n",
      "        [14208.3857],\n",
      "        [10520.3809],\n",
      "        ...,\n",
      "        [15771.0693],\n",
      "        [ 6852.4502],\n",
      "        [11336.2012]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[165/1000]\n",
      " - TRAIN LOSS : 154.32933814335988 R2 : 0.9993963903056596\n",
      "- VAL LOSS : 169.74581909179688 R2 : 0.9998493790626526\n",
      "성능 및 손실 개선이 없어서 164 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10562.2070],\n",
      "        [14206.9434],\n",
      "        [10510.3262],\n",
      "        ...,\n",
      "        [15778.8779],\n",
      "        [ 6857.3472],\n",
      "        [11334.1123]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[166/1000]\n",
      " - TRAIN LOSS : 154.103800235236 R2 : 0.9993984469378943\n",
      "- VAL LOSS : 169.754638671875 R2 : 0.999851405620575\n",
      "성능 및 손실 개선이 없어서 165 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10565.9014],\n",
      "        [14203.4307],\n",
      "        [10506.1543],\n",
      "        ...,\n",
      "        [15764.5352],\n",
      "        [ 6858.9053],\n",
      "        [11330.5518]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[167/1000]\n",
      " - TRAIN LOSS : 153.64821466597616 R2 : 0.9993975880870297\n",
      "- VAL LOSS : 169.65646362304688 R2 : 0.9998480677604675\n",
      "성능 및 손실 개선이 없어서 166 EPOCH에 학습 중단\n",
      "pre_val : tensor([[10563.7031],\n",
      "        [14208.1738],\n",
      "        [10513.4482],\n",
      "        ...,\n",
      "        [15766.0312],\n",
      "        [ 6845.5762],\n",
      "        [11339.6279]])\n",
      " y_val:        closing_price_10\n",
      "34091              10560\n",
      "29155              14405\n",
      "122527             10585\n",
      "137017             12965\n",
      "11348              10715\n",
      "...                  ...\n",
      "86949               7270\n",
      "107611             12595\n",
      "115648             16125\n",
      "50244               6800\n",
      "22221              11385\n",
      "\n",
      "[13634 rows x 1 columns]\n",
      "[168/1000]\n",
      " - TRAIN LOSS : 153.6695510207752 R2 : 0.9993994505340148\n",
      "- VAL LOSS : 169.82508850097656 R2 : 0.9998471736907959\n",
      "성능 및 손실 개선이 없어서 167 EPOCH에 학습 중단\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 29\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(valDF, valtargetDF, model)\u001b[0m\n\u001b[0;32m     26\u001b[0m     score_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m val_loss, val_r2, pre_val \u001b[38;5;241m=\u001b[39m testing(valDF, valtargetDF, model)\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KDP-2\\anaconda3\\envs\\TORCH_CV_38\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, r2 = training(X_val, y_val, rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_CV_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
